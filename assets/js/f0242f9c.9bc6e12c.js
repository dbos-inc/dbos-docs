"use strict";(self.webpackChunkdbos_docs=self.webpackChunkdbos_docs||[]).push([[4419],{6889:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>i,default:()=>d,frontMatter:()=>r,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"python/examples/chatbot","title":"LLM-Powered Chatbot","description":"In this example, you\'ll learn how to build an interactive LLM-powered chatbot with DBOS and LangChain and serverlessly deploy it to DBOS Cloud.","source":"@site/docs/python/examples/chatbot.md","sourceDirName":"python/examples","slug":"/python/examples/chatbot","permalink":"/python/examples/chatbot","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":7,"frontMatter":{"displayed_sidebar":"examplesSidebar","sidebar_position":7,"title":"LLM-Powered Chatbot"},"sidebar":"examplesSidebar","previous":{"title":"AI-Powered Slackbot","permalink":"/python/examples/rag-slackbot"},"next":{"title":"Reliable Customer Service Agent","permalink":"/python/examples/customer-service"}}');var a=t(4848),o=t(8453);const r={displayed_sidebar:"examplesSidebar",sidebar_position:7,title:"LLM-Powered Chatbot"},i=void 0,l={},c=[{value:"Import and Initialize the App",id:"import-and-initialize-the-app",level:2},{value:"Setting Up LangChain",id:"setting-up-langchain",level:2},{value:"Handling Chats",id:"handling-chats",level:2},{value:"Tracking App Usage",id:"tracking-app-usage",level:2},{value:"Try it Yourself!",id:"try-it-yourself",level:2},{value:"Creating an OpenAI Account",id:"creating-an-openai-account",level:3},{value:"Deploying to the Cloud",id:"deploying-to-the-cloud",level:3},{value:"Running Locally",id:"running-locally",level:3}];function h(e){const n={a:"a",code:"code",h2:"h2",h3:"h3",li:"li",ol:"ol",p:"p",pre:"pre",...(0,o.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.p,{children:"In this example, you'll learn how to build an interactive LLM-powered chatbot with DBOS and LangChain and serverlessly deploy it to DBOS Cloud."}),"\n",(0,a.jsxs)(n.p,{children:["You can see the chatbot live ",(0,a.jsx)(n.a,{href:"https://demo-chatbot.cloud.dbos.dev/",children:"here"}),"."]}),"\n",(0,a.jsxs)(n.p,{children:["In addition to chatting, this bot displays both the amount of CPU time and wall-clock time consumed by your requests.\nAs you chat, you'll quickly notice that while your requests may take a long time, they consume very little CPU.\nThat's because they spend most of their time idle waiting for the LLM to respond.\nThis gap explains why DBOS is ",(0,a.jsx)(n.a,{href:"https://www.dbos.dev/blog/aws-lambda-hidden-wait-costs",children:"50x more cost-efficient"})," than other serverless platforms for AI workloads\u2014because DBOS bills only for the CPU time you actually consume, while other platforms bill for the total request duration."]}),"\n",(0,a.jsxs)(n.p,{children:["All source code is ",(0,a.jsx)(n.a,{href:"https://github.com/dbos-inc/dbos-demo-apps/tree/main/python/chatbot",children:"available on GitHub"}),"."]}),"\n",(0,a.jsx)(n.h2,{id:"import-and-initialize-the-app",children:"Import and Initialize the App"}),"\n",(0,a.jsx)(n.p,{children:"Let's start off with imports and initializing DBOS.\nWe'll also set up FastAPI to serve HTTP requests."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import os\nimport threading\nimport time\nfrom collections import deque\n\nimport psutil\nfrom dbos import DBOS, DBOSConfig\nfrom fastapi import FastAPI\nfrom fastapi.responses import HTMLResponse\nfrom langchain_core.messages import HumanMessage\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\nfrom langchain_openai import ChatOpenAI\nfrom langgraph.checkpoint.postgres import PostgresSaver\nfrom langgraph.graph import START, MessagesState, StateGraph\nfrom psycopg_pool import ConnectionPool\nfrom pydantic import BaseModel\n\nfrom .schema import chat_history\n\napp = FastAPI()\nconfig: DBOSConfig = {\n    "name": "chatbot",\n    "database_url": os.environ.get(\'DBOS_DATABASE_URL\'),\n}\ndbos = DBOS(fastapi=app, config=config)\n'})}),"\n",(0,a.jsx)(n.h2,{id:"setting-up-langchain",children:"Setting Up LangChain"}),"\n",(0,a.jsxs)(n.p,{children:["Next, let's set up Langchain.\nWe'll use Langchain to answer each chat message using OpenAI's ",(0,a.jsx)(n.code,{children:"gpt-3.5-turbo"})," model.\nWe'll configure LangChain to store message history in Postgres so it persists across app restarts."]}),"\n",(0,a.jsx)(n.p,{children:"For fun, let's also instruct our chatbot to talk like a pirate."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'def create_langchain():\n    # We use gpt-3.5-turbo as our model.\n    model = ChatOpenAI(model="gpt-3.5-turbo")\n\n    # This prompt instructs the model how to act. We\'ll tell it to talk like a pirate!\n    prompt = ChatPromptTemplate.from_messages(\n        [\n            (\n                "system",\n                "You talk like a pirate. Answer all questions to the best of your ability.",\n            ),\n            MessagesPlaceholder(variable_name="messages"),\n        ]\n    )\n\n    # This function tells LangChain to invoke our model with our prompt.\n    def call_model(state: MessagesState):\n        chain = prompt | model\n        response = chain.invoke(state)\n        return {"messages": response}\n\n    # Create a checkpointer LangChain can use to store message history in Postgres.\n    db = DBOS.config["database"]\n    connection_string = f"postgresql://{db[\'username\']}:{db[\'password\']}@{db[\'hostname\']}:{db[\'port\']}/{db[\'app_db_name\']}"\n    pool = ConnectionPool(connection_string)\n    checkpointer = PostgresSaver(pool)\n\n    # Finally, construct and return the graph LangChain uses to respond to each message.\n    # This chatbot uses a simple one-node graph that just calls the model.\n    graph = StateGraph(state_schema=MessagesState)\n    graph.add_node("model", call_model)\n    graph.add_edge(START, "model")\n    return graph.compile(checkpointer=checkpointer)\n\n\nchain = create_langchain()\n'})}),"\n",(0,a.jsx)(n.h2,{id:"handling-chats",children:"Handling Chats"}),"\n",(0,a.jsx)(n.p,{children:"Now, let's chat!\nWe'll first write the endpoint that handles each chat request."}),"\n",(0,a.jsx)(n.p,{children:"This endpoint is a DBOS workflow with three steps:"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsx)(n.li,{children:"Store the incoming chat message in Postgres."}),"\n",(0,a.jsx)(n.li,{children:"Use LangChain to query the LLM to respond to the chat message."}),"\n",(0,a.jsx)(n.li,{children:"Store the response in Postgres."}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"It also records the total duration of each request in an in-memory buffer."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'class ChatSchema(BaseModel):\n    message: str\n    username: str\n\n\n@app.post("/chat")\n@DBOS.workflow()\ndef chat_workflow(chat: ChatSchema):\n    start_time = time.time()\n    insert_chat(chat.username, chat.message, True)\n    response = query_model(chat.message, chat.username)\n    insert_chat(chat.username, response, False)\n    elapsed_time = time.time() - start_time\n    wallclock_times_buffer.append((time.time(), elapsed_time))\n    return {"content": response, "isUser": True}\n'})}),"\n",(0,a.jsx)(n.p,{children:"Next, let's write the function that actually queries LangChain for each new message.\nIt uses your username as a thread ID so different users can have different threads of conversation."}),"\n",(0,a.jsxs)(n.p,{children:["We annotate this function with ",(0,a.jsx)(n.code,{children:"@DBOS.step()"})," to mark it as a step in our chat workflow."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'@DBOS.step()\ndef query_model(message: str, username: str) -> str:\n    config = {"configurable": {"thread_id": username}}\n    input_messages = [HumanMessage(message)]\n    output = chain.invoke({"messages": input_messages}, config)\n    return output["messages"][-1].content\n'})}),"\n",(0,a.jsx)(n.p,{children:"We also need a history endpoint that retrieves all past chats from the database for a particular user."}),"\n",(0,a.jsx)(n.p,{children:"This function is called when we start the chatbot so it can display your chat history."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'@app.get("/history/{username}")\ndef history_endpoint(username: str):\n    return get_chats(username)\n'})}),"\n",(0,a.jsxs)(n.p,{children:["Then, let's use SQLAlchemy to write the functions that write chats to and read chats from the database.\nWe annotate these functions with ",(0,a.jsx)(n.code,{children:"@DBOS.transaction()"})," to access DBOS's managed database connection."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'@DBOS.transaction()\ndef insert_chat(username: str, content: str, is_user: bool):\n    DBOS.sql_session.execute(\n        chat_history.insert().values(\n            username=username, content=content, is_user=is_user\n        )\n    )\n\n\n@DBOS.transaction()\ndef get_chats(username: str):\n    stmt = (\n        chat_history.select()\n        .where(chat_history.c.username == username)\n        .order_by(chat_history.c.created_at.asc())\n    )\n    result = DBOS.sql_session.execute(stmt)\n    return [{"content": row.content, "isUser": row.is_user} for row in result]\n'})}),"\n",(0,a.jsx)(n.p,{children:"Additionally, let's serve the app's frontend from an HTML file using FastAPI.\nIn production, we recommend using DBOS primarily for the backend, with your frontend deployed elsewhere."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'@app.get("/")\ndef frontend():\n    with open(os.path.join("html", "app.html")) as file:\n        html = file.read()\n    return HTMLResponse(html)\n'})}),"\n",(0,a.jsx)(n.h2,{id:"tracking-app-usage",children:"Tracking App Usage"}),"\n",(0,a.jsx)(n.p,{children:"Finally, let's write some code to track the CPU time and wall-clock time consumed by your requests so we can display those metrics in the app's UI.\nThis code runs once a second in a background thread."}),"\n",(0,a.jsxs)(n.p,{children:["We track the CPU consumption of this process using ",(0,a.jsx)(n.code,{children:"psutil"}),".\nWe track wall-clock time by recording the end-to-end duration of each request."]}),"\n",(0,a.jsx)(n.p,{children:"When you first start the app, you'll notice some small residual CPU consumption from the HTTP server.\nHowever, as you start chatting, you'll quickly see that each chat only consumes ~10ms of CPU time, but 1-2 seconds of wall-clock time.\nThis gap explains why DBOS is 50x cheaper than other serverless platforms for AI workloads\u2014because DBOS bills only for the CPU time you actually consume, while other platforms bill for the total request duration."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'last_cpu_time_ms = 0\ncpu_times_buffer = deque()\nwallclock_times_buffer = deque()\n\n\ndef update_cpu_usage():\n    while True:\n        time.sleep(1)\n        global last_cpu_time_ms\n        # Every second, record CPU time consumed by this process\n        # in the last second.\n        process = psutil.Process()\n        cpu_times = process.cpu_times()\n        cpu_time = cpu_times.system + cpu_times.user\n        time_consumed = cpu_time - last_cpu_time_ms\n        if last_cpu_time_ms > 0:\n            cpu_times_buffer.append((time.time(), time_consumed))\n        last_cpu_time_ms = cpu_time\n        # We only track usage in the last minute, so\n        # pop measurements more than 60 seconds old.\n        for buf in [cpu_times_buffer, wallclock_times_buffer]:\n            while buf and time.time() - buf[0][0] > 60:\n                buf.popleft()\n\n\nthreading.Thread(target=update_cpu_usage).start()\n\n\n@app.get("/times")\ndef times_endpoint():\n    return {\n        "cpu_time": sum([t for _, t in cpu_times_buffer]),\n        "wall_clock_time": sum([t for _, t in wallclock_times_buffer]),\n    }\n\n'})}),"\n",(0,a.jsx)(n.h2,{id:"try-it-yourself",children:"Try it Yourself!"}),"\n",(0,a.jsx)(n.h3,{id:"creating-an-openai-account",children:"Creating an OpenAI Account"}),"\n",(0,a.jsxs)(n.p,{children:["To run this app, you need an OpenAI developer account.\nObtain an API key ",(0,a.jsx)(n.a,{href:"https://platform.openai.com/api-keys",children:"here"})," and set up a payment method for your account ",(0,a.jsx)(n.a,{href:"https://platform.openai.com/account/billing/overview",children:"here"}),".\nThis bot uses ",(0,a.jsx)(n.code,{children:"gpt-3.5-turbo"})," for text generation.\nMake sure you have some credits (~$1) to use it."]}),"\n",(0,a.jsx)(n.p,{children:"Set your API key as an environment variable:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-shell",children:"export OPENAI_API_KEY=<your_openai_key>\n"})}),"\n",(0,a.jsx)(n.h3,{id:"deploying-to-the-cloud",children:"Deploying to the Cloud"}),"\n",(0,a.jsx)(n.p,{children:"To deploy this app to DBOS Cloud, first install the DBOS Cloud CLI (requires Node):"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-shell",children:"npm i -g @dbos-inc/dbos-cloud\n"})}),"\n",(0,a.jsxs)(n.p,{children:["Then clone the ",(0,a.jsx)(n.a,{href:"https://github.com/dbos-inc/dbos-demo-apps",children:"dbos-demo-apps"})," repository and deploy:"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-shell",children:"git clone https://github.com/dbos-inc/dbos-demo-apps.git\ncd python/chatbot\ndbos-cloud app deploy\n"})}),"\n",(0,a.jsxs)(n.p,{children:["This command outputs a URL\u2014visit it to see your chatbot!\nYou can also visit the ",(0,a.jsx)(n.a,{href:"https://console.dbos.dev/login-redirect",children:"DBOS Cloud Console"})," to see your app's status and logs."]}),"\n",(0,a.jsx)(n.h3,{id:"running-locally",children:"Running Locally"}),"\n",(0,a.jsxs)(n.p,{children:["First, clone and enter the ",(0,a.jsx)(n.a,{href:"https://github.com/dbos-inc/dbos-demo-apps",children:"dbos-demo-apps"})," repository:"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-shell",children:"git clone https://github.com/dbos-inc/dbos-demo-apps.git\ncd python/chatbot\n"})}),"\n",(0,a.jsx)(n.p,{children:"Then create a virtual environment:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-shell",children:"python3 -m venv .venv\nsource .venv/bin/activate\n"})}),"\n",(0,a.jsx)(n.p,{children:"DBOS requires a Postgres database.\nIf you don't already have one, you can start one with Docker:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-shell",children:"export PGPASSWORD=dbos\npython3 start_postgres_docker.py\n"})}),"\n",(0,a.jsx)(n.p,{children:"Then start your app:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-shell",children:"pip install -r requirements.txt\ndbos migrate\ndbos start\n"})}),"\n",(0,a.jsxs)(n.p,{children:["Visit ",(0,a.jsx)(n.a,{href:"http://localhost:8000",children:(0,a.jsx)(n.code,{children:"http://localhost:8000"})})," to see your chatbot!"]})]})}function d(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(h,{...e})}):h(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>r,x:()=>i});var s=t(6540);const a={},o=s.createContext(a);function r(e){const n=s.useContext(o);return s.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function i(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:r(e.components),s.createElement(o.Provider,{value:n},e.children)}}}]);