"use strict";(self.webpackChunkdbos_docs=self.webpackChunkdbos_docs||[]).push([[4492],{9611:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>i,default:()=>d,frontMatter:()=>r,metadata:()=>o,toc:()=>u});const o=JSON.parse('{"id":"python/prompting","title":"AI-Assisted Development","description":"If you\'re using an AI coding agent to build a DBOS application, make sure it has the latest information on DBOS by either:","source":"@site/docs/python/prompting.md","sourceDirName":"python","slug":"/python/prompting","permalink":"/python/prompting","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":30,"frontMatter":{"sidebar_position":30,"title":"AI-Assisted Development"},"sidebar":"tutorialSidebar","previous":{"title":"Add DBOS To Your App","permalink":"/python/integrating-dbos"},"next":{"title":"Workflows","permalink":"/python/tutorials/workflow-tutorial"}}');var s=t(4848),a=t(8453);const r={sidebar_position:30,title:"AI-Assisted Development"},i=void 0,l={},u=[{value:"DBOS Agent Skills",id:"dbos-agent-skills",level:2},{value:"DBOS Prompt",id:"dbos-prompt",level:2}];function c(e){const n={a:"a",code:"code",h2:"h2",li:"li",ol:"ol",p:"p",pre:"pre",ul:"ul",...(0,a.R)(),...e.components},{Details:t}=n;return t||function(e,n){throw new Error("Expected "+(n?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("Details",!0),(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.p,{children:"If you're using an AI coding agent to build a DBOS application, make sure it has the latest information on DBOS by either:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"#dbos-agent-skills",children:"Installing DBOS skills."})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"#dbos-prompt",children:"Providing your agent with a DBOS prompt."})}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["You may also want to use the ",(0,s.jsx)(n.a,{href:"/integrations/mcp",children:"DBOS MCP server"})," so your model can directly access your application's workflows and steps."]}),"\n",(0,s.jsx)(n.h2,{id:"dbos-agent-skills",children:"DBOS Agent Skills"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.a,{href:"https://agentskills.io/home",children:"Agent Skills"})," help developers use AI agents to add DBOS durable workflows to their applications.\nDBOS provides open-source skills you can check out ",(0,s.jsx)(n.a,{href:"https://github.com/dbos-inc/agent-skills",children:"here"}),"."]}),"\n",(0,s.jsx)(n.p,{children:"To install them into your coding agent, run:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"npx skills add dbos-inc/agent-skills\n"})}),"\n",(0,s.jsxs)(n.p,{children:["The ",(0,s.jsx)(n.a,{href:"https://skills.sh/",children:"Skills CLI"})," is compatible with most coding agents, including Claude Code, Codex, Antigravity, and Cursor."]}),"\n",(0,s.jsx)(n.h2,{id:"dbos-prompt",children:"DBOS Prompt"}),"\n",(0,s.jsx)(n.p,{children:"You can use this prompt to add rich information about DBOS to your AI coding agent's context.\nYou can copy and paste it directly into your context, or follow these directions to add it to your AI-powered IDE or coding agent of choice:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Claude Code: Add the prompt, or a link to it, to your CLAUDE.md file."}),"\n",(0,s.jsxs)(n.li,{children:["Cursor: Add the prompt to ",(0,s.jsx)(n.a,{href:"https://docs.cursor.com/context/rules-for-ai",children:"your project rules"}),"."]}),"\n",(0,s.jsxs)(n.li,{children:["GitHub Copilot: Create a ",(0,s.jsx)(n.a,{href:"https://docs.github.com/en/copilot/customizing-copilot/adding-repository-custom-instructions-for-github-copilot",children:(0,s.jsx)(n.code,{children:".github/copilot-instructions.md"})})," file in your repository and add the prompt to it."]}),"\n"]}),"\n",(0,s.jsxs)(t,{children:[(0,s.jsx)("summary",{children:(0,s.jsx)("strong",{children:"DBOS Python Prompt"})}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-markdown",children:'# Build Reliable Applications With DBOS\n\n## Guidelines\n\n- Respond in a friendly and concise manner\n- Ask clarifying questions when requirements are ambiguous\n- Generate code in Python using the DBOS library\n- You MUST import all methods and classes used in the code you generate\n- You SHALL keep all code in a single file unless otherwise specified.\n- DBOS does NOT stand for anything.\n\n## Workflow Guidelines\n\nWorkflows provide durable execution so you can write programs that are resilient to any failure.\nWorkflows are comprised of steps, which are ordinary Python functions annotated with @DBOS.step().\nWhen using DBOS workflows, you should annotate any function that performs complex operations or accesses external APIs or services as a step. \nYou can turn any Python function into a step by annotating it with the @DBOS.step decorator. The only requirement is that its inputs and outputs should be serializable.\n\nIf a workflow is interrupted for any reason (e.g., an executor restarts or crashes), when your program restarts the workflow automatically resumes execution from the last completed step.\n\n- If asked to add DBOS to existing code, you MUST ask which function to make a workflow. Do NOT recommend any changes until they have told you what function to make a workflow. Do NOT make a function a workflow unless SPECIFICALLY requested.\n- When making a function a workflow, you should make all functions it calls steps. Do NOT change the functions in any way except by adding the @Step annotation.\n- Do NOT make functions steps unless they are DIRECTLY called by a workflow.\n- If the workflow function performs a non-deterministic action, you MUST move that action to its own function and make that function a step. Examples of non-deterministic actions include accessing an external API or service, accessing files on disk, generating a random number, of getting the current time.\n- Do NOT use threads to start workflows or to start steps in workflows. You should instead use DBOS.start_workflow and DBOS queues.\n- DBOS workflows and steps should NOT have side effects in memory outside of their own scope. They can access global variables, but they should NOT create or update global variables or variables outside their scope.\n- Do NOT call DBOS.start_workflow or DBOS.recv from a step\n- Do NOT start workflows from inside a step.\n- Do NOT call DBOS.set_event and DBOS.recv from outside a workflow.\n\n## DBOS Lifecycle Guidelines\n\nA DBOS application MUST always be configured like so, unless otherwise specified, configuring and launching DBOS in its main function:\n\n```python\nif __name__ == "__main__":\n    config: DBOSConfig = {\n        "name": "my-app",\n        "system_database_url": os.environ.get("DBOS_SYSTEM_DATABASE_URL"),\n    }\n    DBOS(config=config)\n    DBOS.launch()\n```\n\nIn a FastAPI application, the server should ALWAYS be started explicitly after a DBOS.launch in the main function:\n\n```python\nif __name__ == "__main__":\n    config: DBOSConfig = {\n        "name": "my-app",\n        "system_database_url": os.environ.get("DBOS_SYSTEM_DATABASE_URL"),\n    }\n    DBOS(config=config)\n    DBOS.launch()\n    uvicorn.run(app, host="0.0.0.0", port=8000)\n```\n\nIf an app contains scheduled workflows and NOTHING ELSE (no HTTP server), then the main thread should block forever while the scheduled workflows run like this:\n\n```python\nif __name__ == "__main__":\n    config: DBOSConfig = {\n        "name": "my-app",\n        "system_database_url": os.environ.get("DBOS_SYSTEM_DATABASE_URL"),\n    }\n    DBOS(config=config)\n    DBOS.launch()\n    threading.Event().wait()\n```\n\nOr if using asyncio:\n\n```python\nimport asyncio\nfrom dbos import DBOS, DBOSConfig\n\nasync def main():\n    config: DBOSConfig = {\n        "name": "dbos-app"\n    }\n    DBOS(config=config)\n    DBOS.launch()\n    await asyncio.Event().wait()\n\nif __name__ == "__main__":\n    asyncio.run(main())\n```\n\n\n## Workflow and Steps Examples\n\nSimple example:\n\n\n```python\nimport os\nfrom dbos import DBOS, DBOSConfig\n\n@DBOS.step()\ndef step_one():\n    print("Step one completed!")\n\n@DBOS.step()\ndef step_two():\n    print("Step two completed!")\n\n@DBOS.workflow()\ndef dbos_workflow():\n    step_one()\n    step_two()\n\nif __name__ == "__main__":\n    config: DBOSConfig = {\n        "name": "dbos-starter",\n        "system_database_url": os.environ.get("DBOS_SYSTEM_DATABASE_URL"),\n    }\n    DBOS(config=config)\n    DBOS.launch()\n    dbos_workflow()\n```\n\nExample with FastAPI:\n\n```python\nimport os\n\nfrom dbos import DBOS, DBOSConfig\nfrom fastapi import FastAPI\n\napp = FastAPI()\n\n@DBOS.step()\ndef step_one():\n    print("Step one completed!")\n\n@DBOS.step()\ndef step_two():\n    print("Step two completed!")\n\n@app.get("/")\n@DBOS.workflow()\ndef dbos_workflow():\n    step_one()\n    step_two()\n\nif __name__ == "__main__":\n    config: DBOSConfig = {\n        "name": "dbos-starter",\n        "system_database_url": os.environ.get("DBOS_SYSTEM_DATABASE_URL"),\n    }\n    DBOS(config=config)\n    DBOS.launch()\n    uvicorn.run(app, host="0.0.0.0", port=8000)\n```\n\nExample with queues:\n\n```python\nimport os\nimport time\n\nfrom dbos import DBOS, DBOSConfig, Queue\nfrom fastapi import FastAPI\n\napp = FastAPI()\n\nqueue = Queue("example-queue")\n\n@DBOS.step()\ndef dbos_step(n: int):\n    time.sleep(5)\n    print(f"Step {n} completed!")\n\n@app.get("/")\n@DBOS.workflow()\ndef dbos_workflow():\n    print("Enqueueing steps")\n    handles = []\n    for i in range(10):\n        handle = queue.enqueue(dbos_step, i)\n        handles.append(handle)\n    results = [handle.get_result() for handle in handles]\n    print(f"Successfully completed {len(results)} steps")\n\nif __name__ == "__main__":\n    config: DBOSConfig = {\n        "name": "dbos-starter",\n        "system_database_url": os.environ.get("DBOS_SYSTEM_DATABASE_URL"),\n    }\n    DBOS(config=config)\n    DBOS.launch()\n    uvicorn.run(app, host="0.0.0.0", port=8000)\n```\n\n### Scheduled Workflows\n\nYou can schedule DBOS workflows to run on a cron schedule. Schedules are stored in the database and can be created, paused, resumed, and deleted at runtime.\n\nA scheduled workflow MUST take two arguments: a `datetime` (the scheduled execution time) and a context object:\n\n```python\nfrom datetime import datetime\nfrom typing import Any\nfrom dbos import DBOS\n\n@DBOS.workflow()\ndef my_periodic_task(scheduled_time: datetime, context: Any):\n    DBOS.logger.info(f"Running task scheduled for {scheduled_time}")\n\nDBOS.create_schedule(\n    schedule_name="my-task-schedule",\n    workflow_fn=my_periodic_task,\n    schedule="*/5 * * * *",  # Every 5 minutes\n)\n```\n\n- Use `DBOS.create_schedule` to create a schedule with a crontab expression.\n- Use `DBOS.pause_schedule` and `DBOS.resume_schedule` to pause and resume schedules.\n- Use `DBOS.delete_schedule` to delete a schedule.\n- Use `DBOS.apply_schedules` to atomically create, update, or delete multiple schedules at once.\n- Use `DBOS.list_schedules` and `DBOS.get_schedule` to inspect schedules.\n- Use `DBOS.backfill_schedule` to enqueue missed executions for a time range.\n- Use `DBOS.trigger_schedule` to immediately trigger a schedule.\n\n\n## Workflow Documentation:\n\n---\nsidebar_position: 10\ntitle: Workflows\ntoc_max_heading_level: 3\n---\n\nWorkflows provide **durable execution** so you can write programs that are **resilient to any failure**.\nWorkflows help you write fault-tolerant background tasks, data processing pipelines, AI agents, and more.\n\nYou can make a function a workflow by annotating it with `@DBOS.workflow()`.\nWorkflows call steps, which are Python functions annotated with `@DBOS.step()`.\nIf a workflow is interrupted for any reason, DBOS automatically recovers its execution from the last completed step.\n\nHere\'s an example of a workflow:\n\n```python\n@DBOS.step()\ndef step_one():\n    print("Step one completed!")\n\n@DBOS.step()\ndef step_two():\n    print("Step two completed!")\n\n@DBOS.workflow()\ndef workflow():\n    step_one()\n    step_two()\n```\n\n## Starting Workflows In The Background\n\nOne common use-case for workflows is building reliable background tasks that keep running even when the program is interrupted, restarted, or crashes.\nYou can use `DBOS.start_workflow` to start a workflow in the background.\nIf you start a workflow this way, it returns a workflow handle, from which you can access information about the workflow or wait for it to complete and retrieve its result.\n\nHere\'s an example:\n\n```python\n@DBOS.workflow()\ndef background_task(input):\n    # ...\n    return output\n\n# Start the background task\nhandle: WorkflowHandle = DBOS.start_workflow(background_task, input)\n# Wait for the background task to complete and retrieve its result.\noutput = handle.get_result()\n```\n\nAfter starting a workflow in the background, you can use `DBOS.retrieve_workflow` to retrieve a workflow\'s handle from its ID.\nYou can also retrieve a workflow\'s handle from outside of your DBOS application with `DBOSClient.retrieve_workflow`.\n\nIf you need to run many workflows in the background and manage their concurrency or flow control, you can also use DBOS queues.\n\n## Workflow IDs and Idempotency\n\nEvery time you execute a workflow, that execution is assigned a unique ID, by default a UUID.\nYou can access this ID through the `DBOS.workflow_id` context variable.\nWorkflow IDs are useful for communicating with workflows and developing interactive workflows.\n\nYou can set the workflow ID of a workflow with `SetWorkflowID`.\nWorkflow IDs must be **globally unique** for your application.\nAn assigned workflow ID acts as an idempotency key: if a workflow is called multiple times with the same ID, it executes only once.\nThis is useful if your operations have side effects like making a payment or sending an email.\nFor example:\n\n```python\n@DBOS.workflow()\ndef example_workflow():\n    DBOS.logger.info(f"I am a workflow with ID {DBOS.workflow_id}")\n\nwith SetWorkflowID("very-unique-id"):\n    example_workflow()\n```\n\n## Determinism\n\nWorkflows are in most respects normal Python functions.\nThey can have loops, branches, conditionals, and so on.\nHowever, a workflow function must be **deterministic**: if called multiple times with the same inputs, it should invoke the same steps with the same inputs in the same order (given the same return values from those steps).\nIf you need to perform a non-deterministic operation like accessing the database, calling a third-party API, generating a random number, or getting the local time, you shouldn\'t do it directly in a workflow function.\nInstead, you should do all database operations in transactions and all other non-deterministic operations in steps.\n\nFor example, **don\'t do this**:\n\n```python\n@DBOS.workflow()\ndef example_workflow():\n    choice = random.randint(0, 1)\n    if choice == 0:\n        step_one()\n    else:\n        step_two()\n```\n\nDo this instead:\n\n```python\n@DBOS.step()\ndef generate_choice():\n    return random.randint(0, 1)\n\n@DBOS.workflow()\ndef example_workflow(friend: str):\n    choice = generate_choice()\n    if choice == 0:\n        step_one()\n    else:\n        step_two()\n```\n\n\n## Workflow Timeouts\n\nYou can set a timeout for a workflow with `SetWorkflowTimeout`.\nWhen the timeout expires, the workflow **and all its children** are cancelled.\nCancelling a workflow sets its status to `CANCELLED` and preempts its execution at the beginning of its next step.\n\nTimeouts are **start-to-completion**: if a workflow is enqueued, the timeout does not begin until the workflow is dequeued and starts execution.\nAlso, timeouts are **durable**: they are stored in the database and persist across restarts, so workflows can have very long timeouts.\n\nExample syntax:\n\n```python\n@DBOS.workflow()\ndef example_workflow():\n    ...\n\n# If the workflow does not complete within 10 seconds, it times out and is cancelled\nwith SetWorkflowTimeout(10):\n    example_workflow()\n```\n\n## Durable Sleep\n\nYou can use `DBOS.sleep()` to put your workflow to sleep for any period of time.\nThis sleep is **durable**&mdash;DBOS saves the wakeup time in the database so that even if the workflow is interrupted and restarted multiple times while sleeping, it still wakes up on schedule.\n\nSleeping is useful for scheduling a workflow to run in the future (even days, weeks, or months from now).\nFor example:\n\n```python\n@DBOS.workflow()\ndef schedule_task(time_to_sleep, task):\n  # Durably sleep for some time before running the task\n  DBOS.sleep(time_to_sleep)\n  run_task(task)\n```\n\n## Debouncing Workflows\n\nYou can create a `Debouncer` to debounce your workflows.\nDebouncing delays workflow execution until some time has passed since the workflow has last been called.\nThis is useful for preventing wasted work when a workflow may be triggered multiple times in quick succession.\nFor example, if a user is editing an input field, you can debounce their changes to execute a processing workflow only after they haven\'t edited the field for some time:\n\n### Debouncer.create\n\n```python\nDebouncer.create(\n    workflow: Callable[P, R],\n    *,\n    debounce_timeout_sec: Optional[float] = None,\n    queue: Optional[Queue] = None,\n) -> Debouncer[P, R]\n```\n\n**Parameters:**\n- `workflow`: The workflow to debounce.\n- `debounce_key`: The debounce key for this debouncer. Used to group workflow executions that will be debounced. For example, if the debounce key is set to customer ID, each customer\'s workflows would be debounced separately.\n- `debounce_timeout_sec`: After this time elapses since the first time a workflow is submitted from this debouncer, the workflow is started regardless of the debounce period.\n- `queue`: When starting a workflow after debouncing, enqueue it on this queue instead of executing it directly.\n\n### debounce\n\n```python\ndebouncer.debounce(\n    debounce_key: str,\n    debounce_period_sec: float,\n    *args: P.args,\n    **kwargs: P.kwargs,\n) -> WorkflowHandle[R]\n```\n\nSubmit a workflow for execution but delay it by `debounce_period_sec`.\nReturns a handle to the workflow.\nThe workflow may be debounced again, which further delays its execution (up to `debounce_timeout_sec`).\nWhen the workflow eventually executes, it uses the **last** set of inputs passed into `debounce`.\n\nAfter the workflow begins execution, the next call to `debounce` starts the debouncing process again for a new workflow execution.\n\n**Parameters:**\n- `debounce_key`: A key used to group workflow executions that will be debounced together. For example, if the debounce key is set to customer ID, each customer\'s workflows would be debounced separately.\n- `debounce_period_sec`: Delay this workflow\'s execution by this period.\n- `*args`: Variadic workflow arguments.\n- `**kwargs`: Variadic workflow keyword arguments.\n\n**Example Syntax**:\n\n```python\n@DBOS.workflow()\ndef process_input(user_input):\n    ...\n\n# Each time a user submits a new input, debounce the process_input workflow.\n# The workflow will wait until 60 seconds after the user stops submitting new inputs,\ndebouncer = Debouncer.create(process_input)\n# then process the last input submitted.\ndef on_user_input_submit(user_id, user_input):\n    debounce_key = user_id\n    debounce_period_sec = 60\n    debouncer.debounce(debounce_key, debounce_period_sec, user_input)\n```\n\n### Debouncer.create_async\n\n```python\nDebouncer.create_async(\n    workflow: Callable[P, Coroutine[Any, Any, R]],\n    *,\n    debounce_timeout_sec: Optional[float] = None,\n    queue: Optional[Queue] = None,\n) -> Debouncer[P, R]\n```\nAsync version of `Debouncer.create`.\n\n### debounce_async\n\n```python\ndebouncer.debounce_async(\n    debounce_key: str,\n    debounce_period_sec: float,\n    *args: P.args,\n    **kwargs: P.kwargs,\n) -> WorkflowHandleAsync[R]:\n```\n\nAsync version of `debouncer.debounce`.\n\n## Coroutine (Async) Workflows\n\nCoroutinues (functions defined with `async def`, also known as async functions) can also be DBOS workflows.\nCoroutine workflows may invoke coroutine steps via await expressions.\nYou should start coroutine workflows using `DBOS.start_workflow_async` and enqueue them using `enqueue_async`.\nCalling a coroutine workflow or starting it with `DBOS.start_workflow_async` always runs it in the same event loop as its caller, but enqueueing it with `enqueue_async` starts the workflow in a different event loop.\nAdditionally, coroutine workflows should use the asynchronous versions of the workflow communication context methods.\n\n\n:::tip\n\nAt this time, DBOS does not support coroutine transactions.\nTo execute transaction functions without blocking the event loop, use `asyncio.to_thread`.\n\n:::\n\n```python\n@DBOS.step()\nasync def example_step():\n    async with aiohttp.ClientSession() as session:\n        async with session.get("https://example.com") as response:\n            return await response.text()\n\n@DBOS.workflow()\nasync def example_workflow(friend: str):\n    await DBOS.sleep_async(10)\n    body = await example_step()\n    result = await asyncio.to_thread(example_transaction, body)\n    return result\n```\n\n### Running Async Steps In Parallel\n\nInitiating several concurrent steps in an `async` workflow, followed by awaiting them with\n`asyncio.gather(..., return_exceptions=True)`, is valid as long as the steps are started\nin a **deterministic order**. For example, the following is allowed:\n\n```python\n    # Start steps in a deterministic order (step1, step2, step3, step4),\n    # then await them all together.\n    tasks = [\n        asyncio.create_task(step1("arg1")),\n        asyncio.create_task(step2("arg2")),\n        asyncio.create_task(step3("arg3")),\n        asyncio.create_task(step4("arg4")),\n    ]\n\n    # Collects exceptions instead of raising immediately\n    results = await asyncio.gather(*tasks, return_exceptions=True)\n    return results\n```\n\nThis is allowed because each step is started in a well-defined sequence before awaiting.\n\nBy contrast, the following is not allowed:\n```python\n    async def seq_a():\n        await step1("arg1")\n        await step2("arg3")\n\n    async def seq_b():\n        await step3("arg2")\n        await step4("arg4")\n\n    tasks = [\n        asyncio.create_task(seq_a()),\n        asyncio.create_task(seq_b()),\n    ]\n\n    results = await asyncio.gather(*tasks, return_exceptions=True)\n    return results\n```\n\nHere, `step2` and `step4` may be started in either order since their execution depends on\nthe relative time taken by `step1` and `step3`.\n\nIf you need to run sequences of operations concurrently, start child workflows and await\ntheir results, rather than interleaving step execution inside a single workflow.\n\nFor proper error handling, when using `asyncio.gather()`, specify `return_exceptions=True`.\nWithout `return_exceptions=True`, `gather` will raise any exception immediately\nand stop awaiting the rest of the tasks. If one of the remaining tasks later fails, its\nexception may go unobserved. Instead, prefer `asyncio.gather(..., return_exceptions=True)`,\nwhich safely waits for all tasks to complete and reports their outcomes.\n\n## Communicating with Workflows\n\nDBOS provides a few different ways to communicate with your workflows.\nYou can:\n\n- Send messages to workflows\n- Publish events from workflows for clients to read\n- Stream values from workflows to clients\n\n\n## Workflow Messaging and Notifications\nYou can send messages to a specific workflow.\nThis is useful for signaling a workflow or sending notifications to it while it\'s running.\n\n### Send\n\n```python\nDBOS.send(\n    destination_id: str,\n    message: Any,\n    topic: Optional[str] = None\n) -> None\n```\n\nYou can call `DBOS.send()` to send a message to a workflow.\nMessages can optionally be associated with a topic and are queued on the receiver per topic.\n\nYou can also call `send` from outside of your DBOS application with the DBOS Client.\n\n### Recv\n\n```python\nDBOS.recv(\n    topic: Optional[str] = None,\n    timeout_seconds: float = 60,\n) -> Any\n```\n\nWorkflows can call `DBOS.recv()` to receive messages sent to them, optionally for a particular topic.\nEach call to `recv()` waits for and consumes the next message to arrive in the queue for the specified topic, returning `None` if the wait times out.\nIf the topic is not specified, this method only receives messages sent without a topic.\n\n### Messages Example\n\nMessages are especially useful for sending notifications to a workflow.\nFor example, in the widget store demo, the checkout workflow, after redirecting customers to a payments page, must wait for a notification that the user has paid.\n\nTo wait for this notification, the payments workflow uses `recv()`, executing failure-handling code if the notification doesn\'t arrive in time:\n\n```python\n@DBOS.workflow()\ndef checkout_workflow():\n  ... # Validate the order, then redirect customers to a payments service.\n  payment_status = DBOS.recv(PAYMENT_STATUS)\n  if payment_status is not None and payment_status == "paid":\n      ... # Handle a successful payment.\n  else:\n      ... # Handle a failed payment or timeout.\n```\n\nAn endpoint waits for the payment processor to send the notification, then uses `send()` to forward it to the workflow:\n\n```python\n@app.post("/payment_webhook/{workflow_id}/{payment_status}")\ndef payment_endpoint(payment_id: str, payment_status: str) -> Response:\n    # Send the payment status to the checkout workflow.\n    DBOS.send(payment_id, payment_status, PAYMENT_STATUS)\n```\n\n### Reliability Guarantees\n\nAll messages are persisted to the database, so if `send` completes successfully, the destination workflow is guaranteed to be able to `recv` it.\nIf you\'re sending a message from a workflow, DBOS guarantees exactly-once delivery.\nIf you\'re sending a message from normal Python code, you can use `SetWorkflowID` with an idempotency key to guarantee exactly-once delivery.\n\n## Workflow Events\n\nWorkflows can publish _events_, which are key-value pairs associated with the workflow.\nThey are useful for publishing information about the status of a workflow or to send a result to clients while the workflow is running.\n\n### set_event\n\n```python\nDBOS.set_event(\n    key: str,\n    value: Any,\n) -> None\n```\n\nAny workflow or step can call `DBOS.set_event` to publish a key-value pair, or update its value if has already been published.\n\n### get_event\n\n```python\nDBOS.get_event(\n    workflow_id: str,\n    key: str,\n    timeout_seconds: float = 60,\n) -> None\n```\n\nYou can call `DBOS.get_event` to retrieve the value published by a particular workflow identity for a particular key.\nIf the event does not yet exist, this call waits for it to be published, returning `None` if the wait times out.\n\nYou can also call `get_event` from outside of your DBOS application with DBOS Client.\n\n### get_all_events\n\n```python\nDBOS.get_all_events(\n    workflow_id: str\n) -> Dict[str, Any]\n```\n\nYou can use `DBOS.get_all_events` to retrieve the latest values of all events published by a workflow.\n\n### Events Example\n\nEvents are especially useful for writing interactive workflows that communicate information to their caller.\nFor example, in the widget store demo, the checkout workflow, after validating an order, needs to send the customer a unique payment ID.\nTo communicate the payment ID to the customer, it uses events.\n\nThe payments workflow emits the payment ID using `set_event()`:\n\n```python\n@DBOS.workflow()\ndef checkout_workflow():\n    ...\n    payment_id = ...\n    dbos.set_event(PAYMENT_ID, payment_id)\n    ...\n```\n\nThe FastAPI handler that originally started the workflow uses `get_event()` to await this payment ID, then returns it:\n\n```python\n@app.post("/checkout/{idempotency_key}")\ndef checkout_endpoint(idempotency_key: str) -> Response:\n    # Idempotently start the checkout workflow in the background.\n    with SetWorkflowID(idempotency_key):\n        handle = DBOS.start_workflow(checkout_workflow)\n    # Wait for the checkout workflow to send a payment ID, then return it.\n    payment_id = DBOS.get_event(handle.workflow_id, PAYMENT_ID)\n    if payment_id is None:\n        raise HTTPException(status_code=404, detail="Checkout failed to start")\n    return Response(payment_id)\n```\n\n### Reliability Guarantees\n\nAll events are persisted to the database, so the latest version of an event is always retrievable.\nAdditionally, if `get_event` is called in a workflow, the retrieved value is persisted in the database so workflow recovery can use that value, even if the event is later updated.\n\n## Workflow Streaming\n\nWorkflows can stream data in real time to clients.\nThis is useful for streaming results from a long-running workflow or LLM call or for monitoring or progress reporting.\n\n<img src={require(\'@site/static/img/workflow-communication/workflow-streams.png\').default} alt="DBOS Steps" width="750" className="custom-img"/>\n\n### Writing to Streams\n\n```python\nDBOS.write_stream(\n    key: str, \n    value: Any\n) -> None:\n```\n\nYou can write values to a stream from a workflow or its steps using `DBOS.write_stream`.\nA workflow may have any number of streams, each identified by a unique key.\n\nWhen you are done writing to a stream, you should close it with `DBOS.close_stream`.\nOtherwise, streams are automatically closed when the workflow terminates.\n\n```python\nDBOS.close_stream(\n    key: str\n) -> None\n```\n\nDBOS streams are immutable and append-only.\nWrites to a stream from a workflow happen exactly-once.\nWrites to a stream from a step happen at-least-once; if a step fails and is retried, it may write to the stream multiple times.\nReaders will see all values written to the stream from all tries of the step in the order in which they were written.\n\n**Example syntax:**\n\n```python\n@DBOS.workflow()\ndef producer_workflow():\n    DBOS.write_stream(example_key, {"step": 1, "data": "value1"})\n    DBOS.write_stream(example_key, {"step": 2, "data": "value2"})\n    DBOS.close_stream(example_key)  # Signal completion\n```\n\n### Reading from Streams\n\n```python\nDBOS.read_stream(\n    workflow_id: str,\n    key: str\n) -> Generator[Any, Any, None]\n```\n\nYou can read values from a stream from anywhere using `DBOS.read_stream`.\nThis function reads values from a stream identified by a workflow ID and key, yielding each value in order until the stream is closed or the workflow terminates.\n\nYou can also read from a stream from outside a DBOS application with a DBOS Client.\n\n**Example syntax:**\n\n```python\nfor value in DBOS.read_stream(workflow_id, example_key):\n    print(f"Received: {value}")\n```\n\n### Configurable Retries\n\nYou can optionally configure a step to automatically retry any exception a set number of times with exponential backoff.\nThis is useful for automatically handling transient failures, like making requests to unreliable APIs.\nRetries are configurable through arguments to the step decorator:\n\n```python\nDBOS.step(\n    retries_allowed: bool = False,\n    interval_seconds: float = 1.0,\n    max_attempts: int = 3,\n    backoff_rate: float = 2.0\n)\n```\n\nFor example, we configure this step to retry exceptions (such as if `example.com` is temporarily down) up to 10 times:\n\n```python\n@DBOS.step(retries_allowed=True, max_attempts=10)\ndef example_step():\n    return requests.get("https://example.com").text\n```\n\n## DBOS Queues\n\nYou can use queues to run many workflows at once with managed concurrency.\nQueues provide _flow control_, letting you manage how many workflows run at once or how often workflows are started.\n\nTo create a queue, specify its name and optional flow control parameters:\n\n```python\nfrom dbos import Queue\n\nQueue(\n    name: str = None,\n    concurrency: Optional[int] = None,\n    limiter: Optional[QueueRateLimit] = None,\n    *,\n    worker_concurrency: Optional[int] = None,\n    priority_enabled: bool = False,\n    partition_queue: bool = False,\n    polling_interval_sec: float = 1.0,\n)\n\nclass QueueRateLimit(TypedDict):\n    limit: int\n    period: float  # In seconds\n```\n\n**Parameters:**\n- `name`: The name of the queue. Must be unique among all queues in the application.\n- `concurrency`: The maximum number of functions from this queue that may run concurrently. This concurrency limit is global across all DBOS processes using this queue. If not provided, any number of functions may run concurrently.\n- `limiter`: A limit on the maximum number of functions which may be started in a given period.\n- `worker_concurrency`: The maximum number of functions from this queue that may run concurrently on a given DBOS process. Must be less than or equal to `concurrency`.\n- `priority_enabled`: Enable setting priority for workflows on this queue.\n- `partition_queue`: Enable partitioning for this queue.\n- `polling_interval_sec`: The interval at which DBOS polls the database for new workflows on this queue.\n\n**Example syntax:**\n\n```python\nfrom dbos import Queue\n\nqueue = Queue("example_queue")\n```\n\nYou can then enqueue any DBOS workflow or step.\nEnqueuing a function submits it for execution and returns a handle to it.\nQueued tasks are started in first-in, first-out (FIFO) order.\n\n```python\nqueue = Queue("example_queue")\n\n@DBOS.workflow()\ndef process_task(task):\n  ...\n\ntask = ...\nhandle = queue.enqueue(process_task, task)\n```\n\n### Queue Example\n\nHere\'s an example of a workflow using a queue to process tasks concurrently:\n\n```python\nfrom dbos import DBOS, Queue\n\nqueue = Queue("example_queue")\n\n@DBOS.workflow()\ndef process_task(task):\n  ...\n\n@DBOS.workflow()\ndef process_tasks(tasks):\n  task_handles = []\n  # Enqueue each task so all tasks are processed concurrently.\n  for task in tasks:\n    handle = queue.enqueue(process_task, task)\n    task_handles.append(handle)\n  # Wait for each task to complete and retrieve its result.\n  # Return the results of all tasks.\n  return [handle.get_result() for handle in task_handles]\n```\n\n### Enqueueing from Another Application\n\nOften, you want to enqueue a workflow from outside your DBOS application.\nFor example, let\'s say you have an API server and a data processing service.\nYou\'re using DBOS to build a durable data pipeline in the data processing service.\nWhen the API server receives a request, it should enqueue the data pipeline for execution on the data processing service.\n\nYou can use the DBOS Client to enqueue workflows from outside your DBOS application by connecting directly to your DBOS application\'s system database.\nSince the DBOS Client is designed to be used from outside your DBOS application, workflow and queue metadata must be specified explicitly.\n\nFor example, this code enqueues the `data_pipeline` workflow on the `pipeline_queue` queue with `task` as an argument.\n\n```python\nfrom dbos import DBOSClient, EnqueueOptions\n\nclient = DBOSClient(system_database_url=os.environ["DBOS_SYSTEM_DATABASE_URL"])\n\noptions: EnqueueOptions = {\n  "queue_name": "pipeline_queue",\n  "workflow_name": "data_pipeline",\n}\nhandle = client.enqueue(options, task)\nresult = handle.get_result()\n```\n\n### Managing Concurrency\n\nYou can control how many workflows from a queue run simultaneously by configuring concurrency limits.\nThis helps prevent resource exhaustion when workflows consume significant memory or processing power.\n\n#### Worker Concurrency\n\nWorker concurrency sets the maximum number of workflows from a queue that can run concurrently on a single DBOS process.\nThis is particularly useful for resource-intensive workflows to avoid exhausting the resources of any process.\nFor example, this queue has a worker concurrency of 5, so each process will run at most 5 workflows from this queue simultaneously:\n\n```python\nfrom dbos import Queue\n\nqueue = Queue("example_queue", worker_concurrency=5)\n```\n\nNote that DBOS uses `executor_id` to distinguish processes&mdash;this is set automatically by Conductor and Cloud, but if those are not used it must be set to a unique value for each process through configuration.\n\n#### Global Concurrency\n\nGlobal concurrency limits the total number of workflows from a queue that can run concurrently across all DBOS processes in your application.\nFor example, this queue will have a maximum of 10 workflows running simultaneously across your entire application.\n\n:::warning\nWorker concurrency limits are recommended for most use cases.\nTake care when using a global concurrency limit as any `PENDING` workflow on the queue counts toward the limit, including workflows from previous application versions\n:::\n\n```python\nfrom dbos import Queue\n\nqueue = Queue("example_queue", concurrency=10)\n```\n\n#### In-Order Processing\n\nYou can use a queue with `concurrency=1` to guarantee sequential, in-order processing of events.\nOnly a single event will be processed at a time.\nFor example, this app processes events sequentially in the order of their arrival:\n\n ```python\nfrom fastapi import FastAPI\nfrom dbos import DBOS, Queue\n\nqueue = Queue("in_order_queue", concurrency=1)\n\n@DBOS.step()\ndef process_event(event: str):\n    ...\n\ndef event_endpoint(event: str):\n    queue.enqueue(process_event, event)\n ```\n\n### Rate Limiting\n\nYou can set _rate limits_ for a queue, limiting the number of functions that it can start in a given period.\nRate limits are global across all DBOS processes using this queue.\nFor example, this queue has a limit of 50 with a period of 30 seconds, so it may not start more than 50 functions in 30 seconds:\n\n```python\nqueue = Queue("example_queue", limiter={"limit": 50, "period": 30})\n```\n\nRate limits are especially useful when working with a rate-limited API, such as many LLM APIs.\n\n\n## Setting Timeouts\n\nYou can set a timeout for an enqueued workflow with `SetWorkflowTimeout`.\nWhen the timeout expires, the workflow **and all its children** are cancelled.\nCancelling a workflow sets its status to `CANCELLED` and preempts its execution at the beginning of its next step.\n\nTimeouts are **start-to-completion**: a workflow\'s timeout does not begin until the workflow is dequeued and starts execution.\nAlso, timeouts are **durable**: they are stored in the database and persist across restarts, so workflows can have very long timeouts.\n\nExample syntax:\n\n```python\n@DBOS.workflow()\ndef example_workflow():\n    ...\n\nqueue = Queue("example-queue")\n\n# If the workflow does not complete within 10 seconds after being dequeued, it times out and is cancelled\nwith SetWorkflowTimeout(10):\n    queue.enqueue(example_workflow)\n```\n\n## Partitioning Queues\n\nYou can **partition** queues to distribute work across dynamically created queue partitions.\nWhen you enqueue a workflow on a partitioned queue, you must supply a queue partition key.\nPartitioned queues dequeue workflows and apply flow control limits for individual partitions, not for the entire queue.\nEssentially, you can think of each partition as a "subqueue" you dynamically create by enqueueing a workflow with a partition key.\n\nFor example, suppose you want your users to each be able to run at most one task at a time.\nYou can do this with a partitioned queue with a maximum concurrency limit of 1 where the partition key is user ID.\n\n**Example Syntax**\n\n```python\nqueue = Queue("partitioned_queue", partition_queue=True, concurrency=1)\n\n@DBOS.workflow()\ndef process_task(task: Task):\n  ...\n\n\ndef on_user_task_submission(user_id: str, task: Task):\n    # Partition the task queue by user ID. As the queue has a\n    # maximum concurrency of 1, this means that at most one\n    # task can run at once per user (but tasks from different\n    # users can run concurrently).\n    with SetEnqueueOptions(queue_partition_key=user_id):\n        queue.enqueue(process_task, task)\n```\n\nSometimes, you want to apply global or per-worker limits to a partitioned queue.\nYou can do this with **multiple levels of queueing**.\nCreate two queues: a partitioned queue with per-partition limits and a non-partitioned queue with global limits.\nEnqueue a "concurrency manager" workflow to the partitioned queue, which then enqueues your actual workflow\nto the non-partitioned queue and awaits its result.\nThis ensures both queues\' flow control limits are enforced on your workflow.\nFor example:\n\n```python\n# By using two levels of queueing, we enforce both a concurrency limit of 1 on each partition\n# and a global concurrency limit of 5, meaning that no more than 5 tasks can run concurrently\n# across all partitions (and at most one task per partition).\nconcurrency_queue = Queue("concurrency-queue", concurrency=5)\npartitioned_queue = Queue("partitioned-queue", partition_queue=True, concurrency=1)\n\ndef on_user_task_submission(user_id: str, task: Task):\n    # First, enqueue a "concurrency manager" workflow to the partitioned\n    # queue to enforce per-partition limits.\n    with SetEnqueueOptions(queue_partition_key=user_id):\n        partitioned_queue.enqueue(concurrency_manager, task)\n\n@DBOS.workflow()\ndef concurrency_manager(task):\n    # The "concurrency manager" workflow enqueues the process_task\n    # workflow on the non-partitioned queue and awaits its results\n    # to enforce global flow control limits.\n    return concurrency_queue.enqueue(process_task, task).get_result()\n\n@DBOS.workflow()\ndef process_task(task):\n    ...\n```\n\n## Deduplication\n\nYou can set a deduplication ID for an enqueued workflow with `SetEnqueueOptions`.\nAt any given time, only one workflow with a specific deduplication ID can be enqueued in the specified queue.\nIf a workflow with a deduplication ID is currently enqueued or actively executing (status `ENQUEUED` or `PENDING`), subsequent workflow enqueue attempt with the same deduplication ID in the same queue will raise a `DBOSQueueDeduplicatedError` exception.\n\nFor example, this is useful if you only want to have one workflow active at a time per user&mdash;set the deduplication ID to the user\'s ID.\n\nExample syntax:\n\n```python\nfrom dbos import DBOS, Queue, SetEnqueueOptions\nfrom dbos import error as dboserror\n\nqueue = Queue("example_queue")\n\nwith SetEnqueueOptions(deduplication_id="my_dedup_id"):\n    try:\n        handle = queue.enqueue(example_workflow, ...)\n    except dboserror.DBOSQueueDeduplicatedError as e:\n        # Handle deduplication error\n```\n\n## Priority\n\nYou can set a priority for an enqueued workflow with `SetEnqueueOptions`.\nWorkflows with the same priority are dequeued in **FIFO (first in, first out)** order. Priority values can range from `1` to `2,147,483,647`, where **a low number indicates a higher priority**.\nIf using priority, you must set `priority_enabled=True` on your queue.\n\n:::tip\nWorkflows without assigned priorities have the highest priority and are dequeued before workflows with assigned priorities.\n:::\n\nExample syntax:\n\n```python\nqueue = Queue("priority_queue", priority_enabled=True)\n\nwith SetEnqueueOptions(priority=10):\n    # All workflows are enqueued with priority set to 10\n    # They will be dequeued in FIFO order\n    for task in tasks:\n        queue.enqueue(task_workflow, task)\n\n# first_workflow (priority=1) will be dequeued before all task_workflows (priority=10)\nwith SetEnqueueOptions(priority=1):\n    queue.enqueue(first_workflow)\n```\n\n## Explicit Queue Listening\n\nBy default, a process running DBOS listens to (dequeues workflows from) all declared queues.\nHowever, sometimes you only want a process to listen to to a specific list of queues.\nYou can use `DBOS.listen_queues` to explicitly tell a process running DBOS to only listen to a specific set of queues.\nYou must call `DBOS.listen_queues` before DBOS is launched.\n\nThis is particularly useful when managing heterogeneous workers, where specific tasks should execute on specific physical servers.\nFor example, say you have a mix of CPU workers and GPU workers and you want CPU tasks to only execute on CPU workers and GPU tasks to only execute on GPU workers.\nYou can create separate queues for CPU and GPU tasks and configure each type of worker to only listen to the appropriate queue:\n\n```python\ncpu_queue = Queue("cpu_queue")\ngpu_queue = Queue("gpu_queue")\n\nif __name__ == "__main__":\n    worker_type = ... # "cpu\' or \'gpu\'\n    config: DBOSConfig = ...\n    DBOS(config=config)\n    if worker_type = "gpu":\n        # GPU workers will only dequeue and execute workflows from the GPU queue\n        DBOS.listen_queues([gpu_queue])\n    elif worker_type == "cpu":\n        # CPU workers will only dequeue and execute workflows from the CPU queue\n        DBOS.listen_queues([cpu_queue])\n    DBOS.launch()\n```\n\nNote that `DBOS.listen_queues` only controls what workflows are dequeued, not what workflows can be enqueued, so you can freely enqueue tasks onto the GPU queue from a CPU worker for execution on a GPU worker, and vice versa.\n\n\n## Python Classes\n\nYou can add DBOS decorators to your Python class instance methods.\nYou can add step decorators to any class methods, but to add a workflow decorator to a class method, its class must inherit from `DBOSConfiguredInstance` and must be decorated with `@DBOS.dbos_class`.\nFor example:\n\n```python\n@DBOS.dbos_class()\nclass URLFetcher(DBOSConfiguredInstance):\n    def __init__(self, url: str):\n        self.url = url\n        super().__init__(instance_name=url)\n\n    @DBOS.workflow()\n    def fetch_workflow(self):\n        return self.fetch_url()\n\n    @DBOS.step()\n    def fetch_url(self):\n        return requests.get(self.url).text\n    \nexample_fetcher = URLFetcher("https://example.com")\nprint(example_fetcher.fetch_workflow())\n```\n\nWhen you create a new instance of a DBOS class,  `DBOSConfiguredInstance` must be instantiated with an `instance_name`.\nThis `instance_name` should be a unique identifier of the instance.\nAdditionally, all DBOS-decorated classes must be instantiated before `DBOS.launch()` is called.\n\nThe reason for these requirements is to enable workflow recovery.\nWhen you create a new instance of a DBOS class, DBOS stores it in a global registry indexed by `instance_name`.\nWhen DBOS needs to recover a workflow belonging to that class, it looks up the class instance using `instance_name` so it can run the workflow using the right instance of its class.\nIf `instance_name` is not supplied, or if DBOS classes are dynamically instantiated after `DBOS.launch()`, then DBOS may not find the class instance it needs to recover a workflow.\n\n\n### Testing DBOS Functions\n\nUnless otherwise specified, recommend testing DBOS applications with pytest.\n\nYou MUST use this fixture to reset DBOS between tests:\n\n```python\n@pytest.fixture()\ndef reset_dbos():\n    DBOS.destroy()\n    config: DBOSConfig = {\n        "name": "my-app",\n        "database_url": os.environ.get("TESTING_DATABASE_URL"),\n    }\n    DBOS(config=config)\n    DBOS.reset_system_database()\n    DBOS.launch()\n```\n\n## Upgrading Workflow Code\n\nOne challenge you may encounter when operating long-running durable workflows in production is **how to deploy breaking changes without disrupting in-progress workflows.**\nA breaking change to a workflow is any change in what steps run or the order in which steps run.\nThe issue is that if a breaking change was made to a workflow, the checkpoints created by a workflow that started on the previous version of the code may not match the steps called by the workflow in the new version of the code, which makes the workflow difficult to recover.\n\nDBOS supports two strategies for safely upgrading workflow code: **patching** and **versioning**.\n\n### Patching\n\nWhen using patching, you use `DBOS.patch()` to make a breaking change in a conditional.\n`DBOS.patch()` returns `True` for new workflows (those started after the breaking change) and `False` for old workflows (those started before the breaking change).\nTherefore, if `DBOS.patch()` is `True`, call the new code, else, call the old code.\n\nTo use patching, you must enable it in configuration:\n\n```python\nconfig: DBOSConfig = {\n    "name": "dbos-app",\n    "system_database_url": os.environ.get("DBOS_SYSTEM_DATABASE_URL"),\n    "enable_patching": True,\n}\nDBOS(config=config)\n```\n\nFor example, let\'s say our workflow is:\n\n```python\n@DBOS.workflow()\ndef workflow():\n  foo()\n  bar()\n```\n\nWe want to replace the call to `foo()` with a call to `baz()`.\nThis is a breaking change because it changes what steps run.\nWe can make this breaking change safely using a patch:\n\n```python\n@DBOS.workflow()\ndef workflow():\n  if DBOS.patch("use-baz"):\n    baz()\n  else:\n    foo()\n  bar()\n```\n\nNow, new workflows will run `baz()`, while old workflows will safely continue through `foo()`.\n\n#### Deprecating and Removing Patches\n\nPatches don\'t need to stay in your code forever.\nOnce all workflows that started before you deployed the patch are complete, you can safely remove patches from your code.\nYou can use the list workflows APIs to see what workflows are still active.\nFirst, you must deprecate the patch with `DBOS.deprecate_patch()`.\nThis safely runs all workflows that contain the patch marker, but does not insert the patch marker into new workflows.\nFor example, here\'s how to deprecate the patch above:\n\n```python\n@DBOS.workflow()\ndef workflow():\n  DBOS.deprecate_patch("use-baz")\n  baz()\n  bar()\n```\n\nThen, when all workflows that started before you deprecated the patch are complete, you can remove the patch entirely:\n\n```python\n@DBOS.workflow()\ndef workflow():\n  baz()\n  bar()\n```\n\nIf any mistakes happen during the process (a breaking change is not patched, or a patch is deprecated or removed prematurely), the workflow will throw a `DBOSUnexpectedStepError` error clearly pointing to the step where the problem occurred.\n\n### Versioning\n\nWhen using versioning, DBOS **versions** applications and workflows.\nAll workflows are tagged with the application version on which they started.\nBy default, application version is automatically computed from a hash of workflow source code.\nHowever, you can set your own version through configuration.\n\n```python\nconfig: DBOSConfig = {\n    "name": "dbos-app",\n    "system_database_url": os.environ.get("DBOS_SYSTEM_DATABASE_URL"),\n    "application_version": "1.0.0",\n}\nDBOS(config=config)\n```\n\nWhen DBOS tries to recover workflows, it only recovers workflows whose version matches the current application version.\nThis prevents unsafe recovery of workflows that depend on different code.\n\nWhen using versioning, we recommend **blue-green** code upgrades.\nWhen deploying a new version of your code, launch new processes running your new code version, but retain some processes running your old code version.\nDirect new traffic to your new processes while your old processes "drain" and complete all workflows of the old code version.\nThen, once all workflows of the old version are complete (you can use `DBOS.list_workflows` to check), you can retire the old code version.\n\n## Workflow Handle\n\nDBOS.start_workflow, DBOS.retrieve_workflow, and enqueue return workflow handles.\n\n#### get_workflow_id\n\n```python\nhandle.get_workflow_id() -> str\n```\n\nRetrieve the ID of the workflow.\n\n#### get_result\n\n```python\nhandle.get_result(\n    *,\n    polling_interval_sec: float = 1.0,\n) -> R\n```\n\nWait for the workflow to complete, then return its result.\n\n**Parameters:**\n- **polling_interval_sec**: The interval at which DBOS polls the database for the workflow\'s result. Only used for enqueued workflows or retrieved handles.\n\n#### get_status\n\n```python\nhandle.get_status() -> WorkflowStatus\n```\n\n## Workflow Management Methods\n\n### list_workflows\n```python\ndef list_workflows(\n    *,\n    workflow_ids: Optional[List[str]] = None,\n    status: Optional[Union[str, List[str]]] = None,\n    start_time: Optional[str] = None,\n    end_time: Optional[str] = None,\n    name: Optional[str] = None,\n    app_version: Optional[str] = None,\n    forked_from: Optional[str] = None,\n    user: Optional[str] = None,\n    queue_name: Optional[str] = None,\n    limit: Optional[int] = None,\n    offset: Optional[int] = None,\n    sort_desc: bool = False,\n    workflow_id_prefix: Optional[str] = None,\n    load_input: bool = True,\n    load_output: bool = True,\n    executor_id: Optional[str] = None,\n    queues_only: bool = False,\n) -> List[WorkflowStatus]:\n```\n\nRetrieve a list of `WorkflowStatus` of all workflows matching specified criteria.\n\n**Parameters:**\n- **workflow_ids**: Retrieve workflows with these IDs.\n- **status**: Retrieve workflows with this status (or one of these statuses) (Must be `ENQUEUED`, `PENDING`, `SUCCESS`, `ERROR`, `CANCELLED`, or `MAX_RECOVERY_ATTEMPTS_EXCEEDED`)\n- **start_time**: Retrieve workflows started after this (RFC 3339-compliant) timestamp.\n- **end_time**: Retrieve workflows started before this (RFC 3339-compliant) timestamp.\n- **name**: Retrieve workflows with this fully-qualified name.\n- **app_version**: Retrieve workflows tagged with this application version.\n- **forked_from**: Retrieve workflows forked from this workflow ID.\n- **user**: Retrieve workflows run by this authenticated user.\n- **queue_name**: Retrieve workflows that were enqueued on this queue.\n- **limit**: Retrieve up to this many workflows.\n- **offset**: Skip this many workflows from the results returned (for pagination).\n- **sort_desc**: Whether to sort the results in descending (`True`) or ascending (`False`) order by workflow start time.\n- **workflow_id_prefix**: Retrieve workflows whose IDs start with the specified string.\n- **load_input**: Whether to load and deserialize workflow inputs. Set to `False` to improve performance when inputs are not needed.\n- **load_output**: Whether to load and deserialize workflow outputs. Set to `False` to improve performance when outputs are not needed.\n- **executor_id**: Retrieve workflows with this executor ID.\n- **queues_only**: If `True`, only retrieve workflows that are currently queued (status `ENQUEUED` or `PENDING` and `queue_name` not null).\n\n### list_queued_workflows\n```python\ndef list_queued_workflows(\n    *,\n    workflow_ids: Optional[List[str]] = None,\n    status: Optional[Union[str, List[str]]] = None,\n    start_time: Optional[str] = None,\n    end_time: Optional[str] = None,\n    name: Optional[str] = None,\n    app_version: Optional[str] = None,\n    forked_from: Optional[str] = None,\n    user: Optional[str] = None,\n    queue_name: Optional[str] = None,\n    limit: Optional[int] = None,\n    offset: Optional[int] = None,\n    sort_desc: bool = False,\n    workflow_id_prefix: Optional[str] = None,\n    load_input: bool = True,\n    load_output: bool = True,\n    executor_id: Optional[str] = None,\n) -> List[WorkflowStatus]:\n```\n\nRetrieve a list of `WorkflowStatus` of all **queued** workflows (status `ENQUEUED` or `PENDING` and `queue_name` not null) matching specified criteria.\n\n**Parameters:**\n- **workflow_ids**: Retrieve workflows with these IDs.\n- **status**: Retrieve workflows with this status (or one of these statuses) (Must be `ENQUEUED` or `PENDING`)\n- **start_time**: Retrieve workflows enqueued after this (RFC 3339-compliant) timestamp.\n- **end_time**: Retrieve workflows enqueued before this (RFC 3339-compliant) timestamp.\n- **name**: Retrieve workflows with this fully-qualified name.\n- **app_version**: Retrieve workflows tagged with this application version.\n- **forked_from**: Retrieve workflows forked from this workflow ID.\n- **user**: Retrieve workflows run by this authenticated user.\n- **queue_name**: Retrieve workflows running on this queue.\n- **limit**: Retrieve up to this many workflows.\n- **offset**: Skip this many workflows from the results returned (for pagination).\n- **sort_desc**: Whether to sort the results in descending (`True`) or ascending (`False`) order by workflow start time.\n- **workflow_id_prefix**: Retrieve workflows whose IDs start with the specified string.\n- **load_input**: Whether to load and deserialize workflow inputs. Set to `False` to improve performance when inputs are not needed.\n- **load_output**: Whether to load and deserialize workflow outputs. Set to `False` to improve performance when outputs are not needed.\n- **executor_id**: Retrieve workflows with this executor ID.\n\n### list_workflow_steps\n```python\ndef list_workflow_steps(\n    workflow_id: str,\n) -> List[StepInfo]\n```\n\nRetrieve the steps of a workflow.\nThis is a list of `StepInfo` objects, with the following structure:\n\n```python\nclass StepInfo(TypedDict):\n    # The unique ID of the step in the workflow. One-indexed.\n    function_id: int\n    # The (fully qualified) name of the step\n    function_name: str\n    # The step\'s output, if any\n    output: Optional[Any]\n    # The error the step threw, if any\n    error: Optional[Exception]\n    # If the step starts or retrieves the result of a workflow, its ID\n    child_workflow_id: Optional[str]\n    # The Unix epoch timestamp at which this step started\n    started_at_epoch_ms: Optional[int]\n    # The Unix epoch timestamp at which this step completed\n    completed_at_epoch_ms: Optional[int]\n```\n\n### cancel_workflow\n\n```python\nDBOS.cancel_workflow(\n    workflow_id: str,\n) -> None\n```\n\nCancel a workflow.\nThis sets is status to `CANCELLED`, removes it from its queue (if it is enqueued) and preempts its execution (interrupting it at the beginning of its next step)\n\n### resume_workflow\n\n```python\nDBOS.resume_workflow(\n    workflow_id: str\n) -> WorkflowHandle[R]\n```\n\nResume a workflow.\nThis immediately starts it from its last completed step.\nYou can use this to resume workflows that are cancelled or have exceeded their maximum recovery attempts.\nYou can also use this to start an enqueued workflow immediately, bypassing its queue.\n\n### fork_workflow\n\n```python\nDBOS.fork_workflow(\n    workflow_id: str,\n    start_step: int,\n    *,\n    application_version: Optional[str] = None,\n) -> WorkflowHandle[R]\n```\n\nStart a new execution of a workflow from a specific step.\nThe input step ID must match the `function_id` of the step returned by `list_workflow_steps`.\nThe specified `start_step` is the step from which the new workflow will start, so any steps whose ID is less than `start_step` will not be re-executed.\n\nThe forked workflow will have a new workflow ID, which can be set with `SetWorkflowID`.\nIt is possible to specify the application version on which the forked workflow will run by setting `application_version`, this is useful for "patching" workflows that failed due to a bug in a previous application version.\n\n### Workflow Status\n\nSome workflow introspection and management methods return a `WorkflowStatus`.\nThis object has the following definition:\n\n```python\nclass WorkflowStatus:\n    # The workflow ID\n    workflow_id: str\n    # The workflow status. Must be one of ENQUEUED, PENDING, SUCCESS, ERROR, CANCELLED, or MAX_RECOVERY_ATTEMPTS_EXCEEDED\n    status: str\n    # The name of the workflow function\n    name: str\n    # The name of the workflow\'s class, if any\n    class_name: Optional[str]\n    # The name with which the workflow\'s class instance was configured, if any\n    config_name: Optional[str]\n    # The user who ran the workflow, if specified\n    authenticated_user: Optional[str]\n    # The role with which the workflow ran, if specified\n    assumed_role: Optional[str]\n    # All roles which the authenticated user could assume\n    authenticated_roles: Optional[list[str]]\n    # The deserialized workflow input object\n    input: Optional[WorkflowInputs]\n    # The workflow\'s output, if any\n    output: Optional[Any]\n    # The error the workflow threw, if any\n    error: Optional[Exception]\n    # Workflow start time, as a Unix epoch timestamp in ms\n    created_at: Optional[int]\n    # Last time the workflow status was updated, as a Unix epoch timestamp in ms\n    updated_at: Optional[int]\n    # If this workflow was enqueued, on which queue\n    queue_name: Optional[str]\n    # The executor to most recently execute this workflow\n    executor_id: Optional[str]\n    # The application version on which this workflow was started\n    app_version: Optional[str]\n    # The start-to-close timeout of the workflow in ms\n    workflow_timeout_ms: Optional[int]\n    # The deadline of a workflow, computed by adding its timeout to its start time.\n    workflow_deadline_epoch_ms: Optional[int]\n    # Unique ID for deduplication on a queue\n    deduplication_id: Optional[str]\n    # Priority of the workflow on the queue, starting from 1 ~ 2,147,483,647. Default 0 (highest priority).\n    priority: Optional[int]\n    # If this workflow is enqueued on a partitioned queue, its partition key\n    queue_partition_key: Optional[str]\n    # If this workflow was forked from another, that workflow\'s ID.\n    forked_from: Optional[str]\n```\n\nRetrieve the workflow status:\n\n\n### Configuring DBOS\n\nTo configure DBOS, pass a `DBOSConfig` object to its constructor.\nFor example:\n\n```python\nconfig: DBOSConfig = {\n    "name": "dbos-example",\n    "system_database_url": os.environ["DBOS_SYSTEM_DATABASE_URL"],\n}\nDBOS(config=config)\n```\n\nThe `DBOSConfig` object has the following fields.\nAll fields except `name` are optional.\n\n```python\nclass DBOSConfig(TypedDict):\n    name: str\n    enable_patching: Optional[bool]\n    application_version: Optional[str]\n    executor_id: Optional[str]\n\n    system_database_url: Optional[str]\n    application_database_url: Optional[str]\n    sys_db_pool_size: Optional[int]\n    dbos_system_schema: Optional[str]\n    system_database_engine: Optional[sqlalchemy.Engine]\n    use_listen_notify: Optional[bool]\n\n    conductor_key: Optional[str]\n    conductor_url: Optional[str]\n\n    enable_otlp: Optional[bool]\n    otlp_traces_endpoints: Optional[List[str]]\n    otlp_logs_endpoints: Optional[List[str]]\n    otlp_attributes: Optional[dict[str, str]]\n    log_level: Optional[str]\n\n    run_admin_server: Optional[bool]\n    admin_port: Optional[int]\n\n    serializer: Optional[Serializer]\n```\n\n- **name**: Your application\'s name.\n- **enable_patching** Enable the patching strategy for safely upgrading workflow code.\n- **application_version**: If using the versioning strategy for safely upgrading workflow code, the code version for this application and its workflows.\n- **executor_id**: A unique process ID used to identify the application instance in distributed environments. If using DBOS Conductor or Cloud, this is set automatically.\n- **system_database_url**: A connection string to your system database.\nThis is the database in which DBOS stores workflow and step state.\nThis may be either Postgres or SQLite, though Postgres is recommended for production.\nDBOS uses this connection string, unmodified, to create a SQLAlchemy Engine\nA valid connection string looks like:\n\n```\npostgresql://[username]:[password]@[hostname]:[port]/[database name]\n```\n\nOr with SQLite:\n\n```\nsqlite:///[path to database file]\n```\n\n:::info\nPasswords in connection strings must be escaped (for example with urllib) if they contain special characters.\n:::\n\nIf no connection string is provided, DBOS uses a SQLite database:\n\n```shell\nsqlite:///[application_name].sqlite\n```\n- **application_database_url**: A connection string to your application database.\nThis is the database in which DBOS executes `@DBOS.transaction` functions.\nThis parameter has the same format and default as `system_database_url`.\nIf you are not using `@DBOS.transaction`, you do not need to supply this parameter.\n- **sys_db_pool_size**: The size of the connection pool used for the DBOS system database. Defaults to 20.\n- **dbos_system_schema**: Postgres schema name for DBOS system tables. Defaults to "dbos".\n- **system_database_engine**: A custom SQLAlchemy engine to use to connect to your system database. If provided, DBOS will not create an engine but use this instead.\n- **use_listen_notify**: Whether to use PostgreSQL LISTEN/NOTIFY (`True`) or polling (`False`) to await notifications and events. Defaults to `True` in Postgres and must be False in SQLite.\n- **conductor_key**: An API key for DBOS Conductor. If provided, application is connected to Conductor. API keys can be created from the DBOS console.\n- **conductor_url**: The URL of the Conductor service to connect to. Only set if you are self-hosting Conductor.\n- **enable_otlp**: Enable DBOS OpenTelemetry tracing and export. Defaults to False.\n- **otlp_traces_endpoints**: DBOS operations automatically generate OpenTelemetry Traces. Use this field to declare a list of OTLP-compatible trace receivers. Requires `enable_otlp` to be True.\n- **otlp_logs_endpoints**: the DBOS logger can export OTLP-formatted log signals. Use this field to declare a list of OTLP-compatible log receivers. Requires `enable_otlp` to be True.\n- **otlp_attributes**: A set of attributes (key-value pairs) to apply to all OTLP-exported logs and traces.\n- **log_level**: Configure the DBOS logger severity. Defaults to `INFO`.\n- **run_admin_server**: Whether to run an HTTP admin server for workflow management operations. Defaults to True.\n- **admin_port**: The port on which the admin server runs. Defaults to 3001.\n- **serializer**: A custom serializer for the system database.\n\n#### Custom Serialization\n\nDBOS must serialize data such as workflow inputs and outputs and step outputs to store it in the system database.\nBy default, data is serialized with `pickle` then Base64-encoded, but you can optionally supply a custom serializer through DBOS configuration.\nA custom serializer must match this interface:\n\n```python\nclass Serializer(ABC):\n\n    @abstractmethod\n    def serialize(self, data: Any) -> str:\n        pass\n\n    @abstractmethod\n    def deserialize(cls, serialized_data: str) -> Any:\n        pass\n```\n\nFor example, here is how to configure DBOS to use a JSON serializer:\n\n```python\nfrom dbos import DBOS, DBOSConfig, Serializer\n\nclass JsonSerializer(Serializer):\n    def serialize(self, data: Any) -> str:\n        return json.dumps(data)\n\n    def deserialize(cls, serialized_data: str) -> Any:\n        return json.loads(serialized_data)\n\nserializer = JsonSerializer()\nconfig: DBOSConfig = {\n    "name": "dbos-starter",\n    "system_database_url": os.environ.get("DBOS_SYSTEM_DATABASE_URL"),\n    "serializer": serializer\n}\nDBOS(config=config)\nDBOS.launch()\n```\n\n### Transactions\n\nTransactions are a special type of step that are optimized for database accesses.\nThey execute as a single database transaction.\n\nONLY use transactions if you are SPECIFICALLY requested to perform database operations, DO NOT USE THEM OTHERWISE.\n\nIf asked to add DBOS to code that already contains database operations, ALWAYS make it a step, do NOT attempt to make it a transaction unless requested.\n\nONLY use transactions with a Postgres database.\nTo access any other database, ALWAYS use steps.\n\nTo make a Python function a transaction, annotate it with the DBOS.transaction decorator.\nThen, access the database using the DBOS.sql_session client, which is a SQLAlchemy client DBOS automatically connects to your database.\nHere are some examples:\n\n\n#### SQLAlchemy\n\n```python\ngreetings = Table(\n    "greetings", \n    MetaData(), \n    Column("name", String), \n    Column("note", String)\n)\n\n@DBOS.transaction()\ndef example_insert(name: str, note: str) -> None:\n    # Insert a new greeting into the database\n    DBOS.sql_session.execute(greetings.insert().values(name=name, note=note))\n\n@DBOS.transaction()\ndef example_select(name: str) -> Optional[str]:\n    # Select the first greeting to a particular name\n    row = DBOS.sql_session.execute(\n        select(greetings.c.note).where(greetings.c.name == name)\n    ).first()\n    return row[0] if row else None\n```\n\n#### Raw SQL\n\n```python\n@DBOS.transaction()\ndef example_insert(name: str, note: str) -> None:\n    # Insert a new greeting into the database\n    sql = text("INSERT INTO greetings (name, note) VALUES (:name, :note)")\n    DBOS.sql_session.execute(sql, {"name": name, "note": note})\n\n\n@DBOS.transaction()\ndef example_select(name: str) -> Optional[str]:\n    # Select the first greeting to a particular name\n    sql = text("SELECT note FROM greetings WHERE name = :name LIMIT 1")\n    row = DBOS.sql_session.execute(sql, {"name": name}).first()\n    return row[0] if row else None\n```\n\nNEVER async def a transaction.\n\n'})})]})]})}function d(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(c,{...e})}):c(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>r,x:()=>i});var o=t(6540);const s={},a=o.createContext(s);function r(e){const n=o.useContext(a);return o.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function i(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:r(e.components),o.createElement(a.Provider,{value:n},e.children)}}}]);