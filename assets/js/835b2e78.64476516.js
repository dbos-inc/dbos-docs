"use strict";(self.webpackChunkdbos_docs=self.webpackChunkdbos_docs||[]).push([[3719],{9170:(e,a,n)=>{n.r(a),n.d(a,{assets:()=>c,contentTitle:()=>r,default:()=>d,frontMatter:()=>i,metadata:()=>o,toc:()=>f});const o=JSON.parse('{"id":"typescript/tutorials/requestsandevents/kafka-integration","title":"Integrating with Kafka","description":"In this guide, you\'ll learn how to use DBOS workflows to process Kafka messages exactly-once.","source":"@site/docs/typescript/tutorials/requestsandevents/kafka-integration.md","sourceDirName":"typescript/tutorials/requestsandevents","slug":"/typescript/tutorials/requestsandevents/kafka-integration","permalink":"/typescript/tutorials/requestsandevents/kafka-integration","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":30,"frontMatter":{"sidebar_position":30,"title":"Integrating with Kafka"},"sidebar":"tutorialSidebar","previous":{"title":"HTTP Serving","permalink":"/typescript/tutorials/requestsandevents/http-serving-tutorial"},"next":{"title":"Creating Custom Event Receivers","permalink":"/typescript/tutorials/requestsandevents/custom-event-receiver"}}');var t=n(4848),s=n(8453);const i={sidebar_position:30,title:"Integrating with Kafka"},r="Integrating with Kafka",c={},f=[{value:"KafkaJS Integration Tutorial",id:"kafkajs-integration-tutorial",level:2},{value:"KafkaJS Integration Reference",id:"kafkajs-integration-reference",level:2},{value:"@Kafka",id:"kafka",level:3},{value:"@KafkaConsume",id:"kafkaconsume",level:3},{value:"Concurrency and Rate Limiting",id:"concurrency-and-rate-limiting",level:3},{value:"Confluent Kafka Integration Tutorial",id:"confluent-kafka-integration-tutorial",level:2},{value:"Confluent Kafka Integration Reference",id:"confluent-kafka-integration-reference",level:2},{value:"@CKafka",id:"ckafka",level:3},{value:"@CKafkaConsume",id:"ckafkaconsume",level:3},{value:"Concurrency and Rate Limiting",id:"concurrency-and-rate-limiting-1",level:3}];function l(e){const a={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",p:"p",pre:"pre",...(0,s.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(a.header,{children:(0,t.jsx)(a.h1,{id:"integrating-with-kafka",children:"Integrating with Kafka"})}),"\n",(0,t.jsx)(a.p,{children:"In this guide, you'll learn how to use DBOS workflows to process Kafka messages exactly-once."}),"\n",(0,t.jsx)(a.p,{children:"DBOS supports two popular Kafka clients: KafkaJS and Confluent Kafka."}),"\n",(0,t.jsx)(a.h2,{id:"kafkajs-integration-tutorial",children:"KafkaJS Integration Tutorial"}),"\n",(0,t.jsxs)(a.p,{children:["First, install the DBOS integration for ",(0,t.jsx)(a.a,{href:"https://kafka.js.org/",children:"KafkaJS"})," in your application:"]}),"\n",(0,t.jsx)(a.pre,{children:(0,t.jsx)(a.code,{children:"npm install @dbos-inc/dbos-kafkajs\n"})}),"\n",(0,t.jsx)(a.p,{children:"Then, define your workflow. It must take in the Kafka topic, partition, and message as inputs:"}),"\n",(0,t.jsx)(a.pre,{children:(0,t.jsx)(a.code,{className:"language-javascript",children:"import { DBOS } from '@dbos-inc/dbos-sdk';\nimport { KafkaConfig, KafkaMessage} from \"kafkajs\";\n\nexport class KafkaExample{\n  @DBOS.workflow()\n  static async kafkaWorkflow(topic: string, partition: number, message: KafkaMessage) {\n    DBOS.logger.info(`Message received: ${message.value?.toString()}`)\n  }\n}\n"})}),"\n",(0,t.jsxs)(a.p,{children:["Then, annotate your method with a ",(0,t.jsx)(a.a,{href:"#kafka-consume",children:(0,t.jsx)(a.code,{children:"@KafkaConsume"})})," decorator specifying which topic to consume from.\nAdditionally, annotate your class with a ",(0,t.jsx)(a.a,{href:"#kafka",children:(0,t.jsx)(a.code,{children:"@Kafka"})})," decorator defining which brokers to connect to.\nDBOS invokes your method exactly-once for each message sent to the topic."]}),"\n",(0,t.jsx)(a.pre,{children:(0,t.jsx)(a.code,{className:"language-javascript",children:'import { DBOS } from "@dbos-inc/dbos-sdk";\nimport { KafkaConfig, KafkaMessage} from "kafkajs";\nimport { Kafka, KafkaConsume } from "@dbos-inc/dbos-kafkajs";\n\nconst kafkaConfig: KafkaConfig = {\n    brokers: [\'localhost:9092\']\n}\n\n@Kafka(kafkaConfig)\nexport class KafkaExample{\n  @KafkaConsume("example-topic")\n  @DBOS.workflow()\n  static async kafkaWorkflow(topic: string, partition: number, message: KafkaMessage) {\n    DBOS.logger.info(`Message received: ${message.value?.toString()}`)\n  }\n}\n'})}),"\n",(0,t.jsxs)(a.p,{children:["If you need more control, you can pass detailed configurations to both the ",(0,t.jsx)(a.code,{children:"@Kafka"})," and ",(0,t.jsx)(a.code,{children:"@KafkaConsume"})," decorators.\nThe ",(0,t.jsx)(a.code,{children:"@Kafka"})," decorator takes in a ",(0,t.jsx)(a.a,{href:"https://kafka.js.org/docs/configuration",children:"KafkaJS configuration object"})," used to configure Kafka for all methods in its class.\nThe ",(0,t.jsx)(a.code,{children:"@KafkaConsume"})," decorator takes in a ",(0,t.jsx)(a.a,{href:"https://kafka.js.org/docs/consuming#options",children:"KafkaJS consumer configuration"})," as an optional second argument.\nFor example, you can specify a custom consumer group ID:"]}),"\n",(0,t.jsx)(a.pre,{children:(0,t.jsx)(a.code,{className:"language-javascript",children:'@KafkaConsume("example-topic", { groupId: "custom-group-id" })\n@DBOS.workflow()\nstatic async kafkaWorkflow(topic: string, partition: number, message: KafkaMessage) {\n  DBOS.logger.info(`Message received: ${message.value?.toString()}`)\n}\n'})}),"\n",(0,t.jsxs)(a.p,{children:["Under the hood, DBOS constructs an ",(0,t.jsx)(a.a,{href:"/typescript/tutorials/workflow-tutorial#workflow-ids-and-idempotency",children:"idempotency key"})," for each Kafka message from its topic, partition, and offset and passes it into your workflow.\nThis combination is guaranteed to be unique for each Kafka cluster.\nThus, even if a message is delivered multiple times (e.g., due to transient network failures or application interruptions), your workflow processes it exactly once."]}),"\n",(0,t.jsx)(a.h2,{id:"kafkajs-integration-reference",children:"KafkaJS Integration Reference"}),"\n",(0,t.jsx)(a.h3,{id:"kafka",children:"@Kafka"}),"\n",(0,t.jsx)(a.p,{children:(0,t.jsx)(a.code,{children:"@Kafka(kafkaConfig: KafkaConfig)"})}),"\n",(0,t.jsxs)(a.p,{children:["Class-level decorator defining a Kafka configuration to use in all class methods.\nTakes in a ",(0,t.jsx)(a.a,{href:"https://kafka.js.org/docs/configuration",children:"KafkaJS configuration object"}),"."]}),"\n",(0,t.jsx)(a.h3,{id:"kafkaconsume",children:"@KafkaConsume"}),"\n",(0,t.jsx)(a.p,{children:(0,t.jsx)(a.code,{children:"@KafkaConsume(topic: string | RegExp | Array<string | RegExp>, consumerConfig?: ConsumerConfig, queueName?: string)"})}),"\n",(0,t.jsxs)(a.p,{children:["Runs a workflow or transaction exactly-once for each message received on the specified topic(s).\nTakes in a Kafka topic or list of Kafka topics (required) and a ",(0,t.jsx)(a.a,{href:"https://kafka.js.org/docs/consuming#options",children:"KafkaJS consumer configuration"})," (optional).\nRequires class to be decorated with ",(0,t.jsx)(a.a,{href:"#kafka",children:(0,t.jsx)(a.code,{children:"@Kafka"})}),".\nThe decorated method must take as input a Kafka topic, partition, and message as in the example below:"]}),"\n",(0,t.jsx)(a.pre,{children:(0,t.jsx)(a.code,{className:"language-javascript",children:'import { DBOS } from "@dbos-inc/dbos-sdk";\nimport { KafkaConfig, KafkaMessage} from "kafkajs";\nimport { Kafka, KafkaConsume } from "@dbos-inc/dbos-kafkajs";\n\nconst kafkaConfig: KafkaConfig = {\n    brokers: [\'localhost:9092\']\n}\n\n@Kafka(kafkaConfig)\nclass KafkaExample{\n\n  @KafkaConsume("example-topic")\n  @DBOS.workflow()\n  static async kafkaWorkflow(topic: string, partition: number, message: KafkaMessage) {\n    // This workflow executes exactly once for each message sent to "example-topic".\n    // All methods annotated with Kafka decorators must take in the topic, partition, and message as inputs just like this method.\n  }\n}\n'})}),"\n",(0,t.jsx)(a.h3,{id:"concurrency-and-rate-limiting",children:"Concurrency and Rate Limiting"}),"\n",(0,t.jsxs)(a.p,{children:["By default, ",(0,t.jsx)(a.code,{children:"@KafkaConsume"})," workflows are started immediately upon receiving Kafka messages.  If ",(0,t.jsx)(a.code,{children:"queueName"})," is provided to the ",(0,t.jsx)(a.code,{children:"@KafkaConsume"})," decorator, then the workflows will be enqueued in a ",(0,t.jsx)(a.a,{href:"/typescript/tutorials/queue-tutorial",children:"workflow queue"})," and subject to rate limits."]}),"\n",(0,t.jsx)(a.h2,{id:"confluent-kafka-integration-tutorial",children:"Confluent Kafka Integration Tutorial"}),"\n",(0,t.jsxs)(a.p,{children:["First, install the DBOS integration for ",(0,t.jsx)(a.a,{href:"https://github.com/confluentinc/confluent-kafka-javascript",children:"Confluent's JavaScript Client for Apache Kafka"})," in your application:"]}),"\n",(0,t.jsx)(a.pre,{children:(0,t.jsx)(a.code,{children:"npm install @dbos-inc/dbos-confluent-kafka\n"})}),"\n",(0,t.jsx)(a.p,{children:"Then, define your workflow. It must take in the Kafka topic, partition, and message as inputs:"}),"\n",(0,t.jsx)(a.pre,{children:(0,t.jsx)(a.code,{className:"language-javascript",children:"import { DBOS } from '@dbos-inc/dbos-sdk';\nimport { KafkaConfig, Message } from \"@dbos-inc/dbos-confluent-kafka\";\n\nexport class CKafkaExample{\n  @DBOS.workflow()\n  static async kafkaWorkflow(topic: string, partition: number, message: Message) {\n    DBOS.logger.info(`Message received: ${message.value?.toString()}`)\n  }\n}\n"})}),"\n",(0,t.jsxs)(a.p,{children:["Then, annotate your method with a ",(0,t.jsx)(a.a,{href:"#ckafka-consume",children:(0,t.jsx)(a.code,{children:"@CKafkaConsume"})})," decorator specifying which topic to consume from.\nAdditionally, annotate your class with a ",(0,t.jsx)(a.a,{href:"#ckafka",children:(0,t.jsx)(a.code,{children:"@CKafka"})})," decorator defining which brokers to connect to.\nDBOS invokes your method exactly-once for each message sent to the topic."]}),"\n",(0,t.jsx)(a.pre,{children:(0,t.jsx)(a.code,{className:"language-javascript",children:'import { DBOS } from "@dbos-inc/dbos-sdk";\nimport { KafkaConfig, Message, CKafka, CKafkaConsume } from "@dbos-inc/dbos-confluent-kafka";\n\nconst kafkaConfig: KafkaConfig = {\n    brokers: [\'localhost:9092\']\n}\n\n@CKafka(kafkaConfig)\nexport class CKafkaExample{\n  @CKafkaConsume("example-topic")\n  @DBOS.workflow()\n  static async kafkaWorkflow(topic: string, partition: number, message: Message) {\n    DBOS.logger.info(`Message received: ${message.value?.toString()}`)\n  }\n}\n'})}),"\n",(0,t.jsxs)(a.p,{children:["If you need more control, you can pass detailed configurations to both the ",(0,t.jsx)(a.code,{children:"@CKafka"})," and ",(0,t.jsx)(a.code,{children:"@CKafkaConsume"})," decorators.\nThe ",(0,t.jsx)(a.code,{children:"@CKafka"})," and ",(0,t.jsx)(a.code,{children:"@CKafkaConsume"})," decorators take in a ",(0,t.jsx)(a.a,{href:"https://github.com/confluentinc/librdkafka/blob/v2.6.1/CONFIGURATION.md",children:"configuration object"})," used to configure Kafka for all methods in its class.  You can also use ",(0,t.jsx)(a.a,{href:"https://github.com/confluentinc/confluent-kafka-javascript/blob/master/MIGRATION.md#kafkajs",children:"KafkaJS-like configuration options"}),"."]}),"\n",(0,t.jsx)(a.p,{children:"For example, you can specify a custom consumer group ID:"}),"\n",(0,t.jsx)(a.pre,{children:(0,t.jsx)(a.code,{className:"language-javascript",children:'@CKafkaConsume("example-topic", { groupId: "custom-group-id" })\n@DBOS.workflow()\nstatic async kafkaWorkflow(topic: string, partition: number, message: Message) {\n  DBOS.logger.info(`Message received: ${message.value?.toString()}`)\n}\n'})}),"\n",(0,t.jsxs)(a.p,{children:["Under the hood, DBOS constructs an ",(0,t.jsx)(a.a,{href:"/typescript/tutorials/workflow-tutorial#workflow-ids-and-idempotency",children:"idempotency key"})," for each Kafka message from its topic, partition, and offset and passes it into your workflow.\nThis combination is guaranteed to be unique for each Kafka cluster.\nThus, even if a message is delivered multiple times (e.g., due to transient network failures or application interruptions), your workflow processes it exactly once."]}),"\n",(0,t.jsx)(a.h2,{id:"confluent-kafka-integration-reference",children:"Confluent Kafka Integration Reference"}),"\n",(0,t.jsx)(a.h3,{id:"ckafka",children:"@CKafka"}),"\n",(0,t.jsx)(a.p,{children:(0,t.jsx)(a.code,{children:"@CKafka(kafkaConfig: KafkaConfig)"})}),"\n",(0,t.jsxs)(a.p,{children:["Class-level decorator defining a Kafka configuration to use in all class methods.\nTakes in a ",(0,t.jsx)(a.a,{href:"https://kafka.js.org/docs/configuration",children:"KafkaJS configuration object"})," or ",(0,t.jsx)(a.a,{href:"https://github.com/confluentinc/librdkafka/blob/v2.6.1/CONFIGURATION.md",children:"rdkafka configuration object"}),"."]}),"\n",(0,t.jsx)(a.h3,{id:"ckafkaconsume",children:"@CKafkaConsume"}),"\n",(0,t.jsx)(a.p,{children:(0,t.jsx)(a.code,{children:"@CKafkaConsume(topic: string | RegExp | Array<string | RegExp>, consumerConfig?: ConsumerConfig, queueName?: string)"})}),"\n",(0,t.jsxs)(a.p,{children:["Runs a workflow or transaction exactly-once for each message received on the specified topic(s).\nTakes in a Kafka topic or list of Kafka topics (required) and a consumer configuration.\nRequires class to be decorated with ",(0,t.jsx)(a.a,{href:"#ckafka",children:(0,t.jsx)(a.code,{children:"@CKafka"})}),".\nThe decorated method must take as input a Kafka topic, partition, and message as in the example below:"]}),"\n",(0,t.jsx)(a.pre,{children:(0,t.jsx)(a.code,{className:"language-javascript",children:'import { DBOS } from "@dbos-inc/dbos-sdk";\nimport { KafkaConfig, Message, CKafka, CKafkaConsume } from "@dbos-inc/dbos-confluent-kafka";\n\nconst kafkaConfig: KafkaConfig = {\n    brokers: [\'localhost:9092\']\n}\n\n@CKafka(kafkaConfig)\nclass CKafkaExample{\n  @CKafkaConsume("example-topic")\n  @DBOS.workflow()\n  static async kafkaWorkflow(topic: string, partition: number, message: KafkaMessage) {\n    // This workflow executes exactly once for each message sent to "example-topic".\n    // All methods annotated with CKafka decorators must take in the topic, partition, and message as inputs just like this method.\n  }\n}\n'})}),"\n",(0,t.jsx)(a.h3,{id:"concurrency-and-rate-limiting-1",children:"Concurrency and Rate Limiting"}),"\n",(0,t.jsxs)(a.p,{children:["By default, ",(0,t.jsx)(a.code,{children:"@CKafkaConsume"})," workflows are started immediately upon receiving Kafka messages.  If ",(0,t.jsx)(a.code,{children:"queueName"})," is provided to the ",(0,t.jsx)(a.code,{children:"@CKafkaConsume"})," decorator, then the workflows will be enqueued in a ",(0,t.jsx)(a.a,{href:"/typescript/tutorials/queue-tutorial",children:"workflow queue"})," and subject to rate limits."]})]})}function d(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,t.jsx)(a,{...e,children:(0,t.jsx)(l,{...e})}):l(e)}},8453:(e,a,n)=>{n.d(a,{R:()=>i,x:()=>r});var o=n(6540);const t={},s=o.createContext(t);function i(e){const a=o.useContext(s);return o.useMemo((function(){return"function"==typeof e?e(a):{...a,...e}}),[a,e])}function r(e){let a;return a=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:i(e.components),o.createElement(s.Provider,{value:a},e.children)}}}]);