"use strict";(self.webpackChunkdbos_docs=self.webpackChunkdbos_docs||[]).push([[7176],{8124:(e,n,o)=>{o.r(n),o.d(n,{assets:()=>c,contentTitle:()=>i,default:()=>f,frontMatter:()=>r,metadata:()=>t,toc:()=>l});const t=JSON.parse('{"id":"python/tutorials/kafka-integration","title":"Integrating with Kafka","description":"Overview of using DBOS with Kafka","source":"@site/docs/python/tutorials/kafka-integration.md","sourceDirName":"python/tutorials","slug":"/python/tutorials/kafka-integration","permalink":"/python/tutorials/kafka-integration","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":8,"frontMatter":{"sidebar_position":8,"title":"Integrating with Kafka","description":"Overview of using DBOS with Kafka"},"sidebar":"tutorialSidebar","previous":{"title":"Logging & Tracing","permalink":"/python/tutorials/logging-and-tracing"},"next":{"title":"Authentication and Authorization","permalink":"/python/tutorials/authentication-authorization"}}');var s=o(4848),a=o(8453);const r={sidebar_position:8,title:"Integrating with Kafka",description:"Overview of using DBOS with Kafka"},i=void 0,c={},l=[{value:"In-Order Processing",id:"in-order-processing",level:2}];function d(e){const n={a:"a",code:"code",h2:"h2",p:"p",pre:"pre",strong:"strong",...(0,a.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.p,{children:"In this guide, you'll learn how to use DBOS transactions and workflows to process Kafka messages with exactly-once semantics."}),"\n",(0,s.jsxs)(n.p,{children:["First, install ",(0,s.jsx)(n.a,{href:"https://docs.confluent.io/kafka-clients/python/current/overview.html",children:"Confluent Kafka"})," in your application:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"pip install confluent-kafka\n"})}),"\n",(0,s.jsx)(n.p,{children:"Then, define your transaction or workflow. It must take in a Kafka message as an input parameter:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from dbos import DBOS, KafkaMessage\n\n@DBOS.workflow()\ndef test_kafka_workflow(msg: KafkaMessage):\n    DBOS.logger.info(f"Message received: {msg.value.decode()}")\n'})}),"\n",(0,s.jsxs)(n.p,{children:["Then, annotate your function with a ",(0,s.jsx)(n.a,{href:"../reference/decorators#kafka_consumer",children:(0,s.jsx)(n.code,{children:"@DBOS.kafka_consumer"})})," decorator specifying which brokers to connect to and which topics to consume from.\nConfiguration setting details are available from the\n",(0,s.jsx)(n.a,{href:"https://docs.confluent.io/platform/current/clients/confluent-kafka-python/html/index.html#pythonclient-configuration",children:"Confluent Kafka API docs"})," and the\n",(0,s.jsx)(n.a,{href:"https://kafka.apache.org/documentation/#consumerconfigs",children:"official Kafka documentation"}),".\nAt a minimum, you must specify ",(0,s.jsx)(n.a,{href:"https://kafka.apache.org/documentation/#consumerconfigs_bootstrap.servers",children:(0,s.jsx)(n.code,{children:"bootstrap.servers"})})," and\n",(0,s.jsx)(n.a,{href:"https://kafka.apache.org/documentation/#consumerconfigs_group.id",children:(0,s.jsx)(n.code,{children:"group.id"})})," configuration settings."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from dbos import DBOS, KafkaMessage\n\n@DBOS.kafka_consumer(\n        config={\n            "bootstrap.servers": "localhost:9092",\n            "group.id": "dbos-kafka-group",\n        },\n        topics=["example-topic"],\n)\n@DBOS.workflow()\ndef test_kafka_workflow(msg: KafkaMessage):\n    DBOS.logger.info(f"Message received: {msg.value.decode()}")\n\n'})}),"\n",(0,s.jsxs)(n.p,{children:["Under the hood, DBOS constructs an ",(0,s.jsx)(n.a,{href:"/python/tutorials/workflow-tutorial#workflow-ids-and-idempotency",children:"idempotency key"})," for each Kafka message from its topic, partition, and offset and passes it into your workflow or transaction.\nThis combination is guaranteed to be unique for each Kafka cluster.\nThus, even if a message is delivered multiple times (e.g., due to transient network failures or application interruptions), your transaction or workflow processes it exactly once."]}),"\n",(0,s.jsx)(n.h2,{id:"in-order-processing",children:"In-Order Processing"}),"\n",(0,s.jsxs)(n.p,{children:["You can process Kafka events in-order by setting ",(0,s.jsx)(n.code,{children:"in_order=True"})," in the ",(0,s.jsx)(n.code,{children:"@DBOS.kafka_consumer"})," decorator.\nIf this is set, messages are processed ",(0,s.jsx)(n.strong,{children:"sequentially"})," in order by offset.\nIn other words, processing of Message #4 does not begin until Message #3 is fully processed.\nFor example:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from dbos import DBOS, KafkaMessage\n\n@DBOS.kafka_consumer(\n        config=config,\n        topics=["example-topic"],\n        in_order=True\n)\n@DBOS.workflow()\ndef process_messages_in_order(msg: KafkaMessage):\n    DBOS.logger.info(f"Messages are processed sequentially in offset order")\n\n'})})]})}function f(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},8453:(e,n,o)=>{o.d(n,{R:()=>r,x:()=>i});var t=o(6540);const s={},a=t.createContext(s);function r(e){const n=t.useContext(a);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function i(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:r(e.components),t.createElement(a.Provider,{value:n},e.children)}}}]);