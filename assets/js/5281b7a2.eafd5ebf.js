"use strict";(self.webpackChunkdbos_docs=self.webpackChunkdbos_docs||[]).push([[2443],{936:(e,o,t)=>{t.r(o),t.d(o,{assets:()=>l,contentTitle:()=>a,default:()=>u,frontMatter:()=>i,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"architecture","title":"DBOS Architecture","description":"DBOS provides a lightweight library for durable workflows built on top of Postgres.","source":"@site/docs/architecture.md","sourceDirName":".","slug":"/architecture","permalink":"/architecture","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{"hide_table_of_contents":false,"title":"DBOS Architecture"},"sidebar":"tutorialSidebar","previous":{"title":"Why DBOS?","permalink":"/why-dbos"},"next":{"title":"Learn DBOS Python","permalink":"/python/programming-guide"}}');var r=t(4848),n=t(8453);const i={hide_table_of_contents:!1,title:"DBOS Architecture"},a=void 0,l={},c=[{value:"Using DBOS in a Distributed Setting",id:"using-dbos-in-a-distributed-setting",level:2},{value:"How DBOS Scales",id:"how-dbos-scales",level:2},{value:"How Workflow Recovery Works",id:"how-workflow-recovery-works",level:2},{value:"Application and Workflow Versions",id:"application-and-workflow-versions",level:2},{value:"Durable Queues",id:"durable-queues",level:2},{value:"Self-Hosting DBOS with Conductor",id:"self-hosting-dbos-with-conductor",level:2},{value:"Host Applications on DBOS Cloud",id:"host-applications-on-dbos-cloud",level:2}];function d(e){const o={a:"a",em:"em",h2:"h2",li:"li",ol:"ol",p:"p",strong:"strong",ul:"ul",...(0,n.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(o.p,{children:"DBOS provides a lightweight library for durable workflows built on top of Postgres."}),"\n",(0,r.jsx)(o.p,{children:"You use DBOS by installing the open-source library into your application and annotating workflows and steps.\nWhile your application runs, DBOS checkpoints those workflows and steps to a Postgres database.\nWhen failures occur, whether from crashes, interruptions, or restarts, DBOS uses those checkpoints to recover each of your workflows from the last completed step."}),"\n",(0,r.jsx)(o.p,{children:"Architecturally, an application built with DBOS looks the below diagram.\nDBOS is implemented entirely in the open-source library you install into your application.\nThe library handles both checkpointing workflows and steps and recovering workflows from failures.\nThere's no orchestration server and no external dependencies except a Postgres database."}),"\n",(0,r.jsx)("img",{src:t(9452).A,alt:"DBOS Architecture",width:"750",className:"custom-img"}),"\n",(0,r.jsxs)(o.p,{children:["To learn more about how to add DBOS to your application, check out the language-specific integration guides (",(0,r.jsx)(o.a,{href:"/python/integrating-dbos",children:"Python"}),", ",(0,r.jsx)(o.a,{href:"/typescript/integrating-dbos",children:"TypeScript"}),", ",(0,r.jsx)(o.a,{href:"/golang/integrating-dbos",children:"Go"}),", ",(0,r.jsx)(o.a,{href:"/java/integrating-dbos",children:"Java"}),")."]}),"\n",(0,r.jsx)(o.h2,{id:"using-dbos-in-a-distributed-setting",children:"Using DBOS in a Distributed Setting"}),"\n",(0,r.jsxs)(o.p,{children:["DBOS naturally scales to a distributed setting with many servers per application and many applications.\nFor example, you might deploy a DBOS application to a Kubernetes cluster, a fleet of EC2 instances, or a serverless platform like Google Cloud Run.\nEach of your application's servers should connect to the same Postgres database, called the system database.\nThis database stores all workflow checkpoints, step outputs, and queue state.\nBy default, each workflow runs on only a single server.\nHowever, you can use mechanisms like ",(0,r.jsx)(o.a,{href:"#durable-queues",children:"durable queues"})," to distribute work across many servers."]}),"\n",(0,r.jsx)(o.p,{children:"Often, you have multiple applications or services that need durable workflows.\nFor example, you might have a service that handles client requests, a service that handles data ingestion, and a service that runs an AI agent.\nYou can separately add DBOS to each of these applications, connecting each to a separate system database to isolate their workflows.\nThis doesn't require multiple Postgres servers\u2014a single physical Postgres server can host multiple system databases, with each database serving a separate DBOS application."}),"\n",(0,r.jsxs)(o.p,{children:["Sometimes, you need to communicate between separate DBOS applications, or between a DBOS application and an application not using DBOS.\nFor example, you might want your API server to enqueue a job on your data processing service.\nYou can use the DBOS Client (",(0,r.jsx)(o.a,{href:"/python/reference/client",children:"Python"}),", ",(0,r.jsx)(o.a,{href:"/typescript/reference/client",children:"TypeScript"}),", ",(0,r.jsx)(o.a,{href:"/golang/reference/client",children:"Go"}),", ",(0,r.jsx)(o.a,{href:"/java/reference/client",children:"Java"}),") to programmatically interact with your application from external code.\nFor example, your API server can create a client connected to your data processing service's system database and use it to enqueue a job, monitor the job's status, and retrieve its result when complete.\nHere's a diagram of what that might look like:"]}),"\n",(0,r.jsx)("img",{src:t(4337).A,alt:"DBOS Architecture",width:"750",className:"custom-img"}),"\n",(0,r.jsx)(o.h2,{id:"how-dbos-scales",children:"How DBOS Scales"}),"\n",(0,r.jsx)(o.p,{children:"You can easily scale a DBOS application by adding more servers to it, so the scalability of DBOS is fundamentally determined by the database it is connected to.\nThe only overhead DBOS adds is database writes: one database write per step (to checkpoint the step's outcome) plus two additional database writes per workflow (one at the beginning to checkpoint workflow inputs, one at the end to checkpoint the workflow outcome)."}),"\n",(0,r.jsx)(o.p,{children:"As these writes are checkpointing workflow inputs and outputs and step outputs, the sizes of the writes are determined by the sizes of your inputs and outputs.\nIf your steps return small objects, the write sizes are negligible, but if they return large files, the write sizes are large.\nThus, we recommend architecting steps to avoid large output sizes (for example, store large files in cloud blob storage like S3 and have steps return pointers to those files)."}),"\n",(0,r.jsxs)(o.p,{children:["While exact numbers depend on the database you are using, a large Postgres database can typically sustain well over 10K writes per second (for example, ",(0,r.jsx)(o.a,{href:"https://planetscale.com/benchmarks/aurora",children:"this benchmark shows 12-18K writes/second"}),"; most large-scale Postgres benchmarks have similar results).\nThus, a DBOS application using a single Postgres database can sustain a throughput of several thousand workflows or steps per second.\nScaling beyond that is possible by sharding workflows across multiple Postgres databases."]}),"\n",(0,r.jsx)(o.h2,{id:"how-workflow-recovery-works",children:"How Workflow Recovery Works"}),"\n",(0,r.jsx)(o.p,{children:"DBOS achieves fault tolerance by checkpointing workflows and steps.\nEvery workflow input and step output is durably stored in the system database.\nWhen workflow execution fails, whether from crashes, network issues, or server restarts, DBOS leverages these checkpoints to recover workflows from their last completed step."}),"\n",(0,r.jsx)(o.p,{children:"Workflow recovery occurs in three steps:"}),"\n",(0,r.jsxs)(o.ol,{children:["\n",(0,r.jsxs)(o.li,{children:["\n",(0,r.jsxs)(o.p,{children:["First, DBOS detects interrupted workflows.\nIn single-node deployments, this happens automatically at startup when DBOS scans for incomplete (PENDING) workflows.\nIn a distributed deployment, some coordination is required, either automatically through services like ",(0,r.jsx)(o.a,{href:"#self-hosting-dbos-with-conductor",children:"DBOS Conductor"})," or ",(0,r.jsx)(o.a,{href:"#host-applications-on-dbos-cloud",children:"DBOS Cloud"}),", or manually using the admin API (detailed ",(0,r.jsx)(o.a,{href:"/production/self-hosting/workflow-recovery",children:"here"}),")."]}),"\n"]}),"\n",(0,r.jsxs)(o.li,{children:["\n",(0,r.jsx)(o.p,{children:"Next, DBOS restarts each interrupted workflow by calling it with its checkpointed inputs.\nAs the workflow re-executes, it checks before each step if that step's output is checkpointed in Postgres.\nIf there is a checkpoint, the step returns the checkpointed output instead of executing."}),"\n"]}),"\n",(0,r.jsxs)(o.li,{children:["\n",(0,r.jsxs)(o.p,{children:["Eventually, the recovered workflow reaches a step with ",(0,r.jsx)(o.strong,{children:"no checkpoint"}),".\nThis marks the point where the original execution failed.\nThe recovered workflow executes that step normally and proceeds from there, thus ",(0,r.jsx)(o.strong,{children:"resuming from the last completed step."})]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(o.p,{children:"For DBOS to be able to safely recover a workflow, your code must satisfy two requirements:"}),"\n",(0,r.jsxs)(o.ol,{children:["\n",(0,r.jsxs)(o.li,{children:["\n",(0,r.jsxs)(o.p,{children:["The workflow function must be ",(0,r.jsx)(o.strong,{children:"deterministic"}),": if executed multiple times, with the same arguments and step return values, the workflow should invoke the same steps with the same inputs in the same order. If you need to perform any non-deterministic operation like accessing the database, calling a third-party API, generating a random number, or getting the local time, you should do it in a step instead of directly in the workflow function."]}),"\n"]}),"\n",(0,r.jsxs)(o.li,{children:["\n",(0,r.jsxs)(o.p,{children:["Steps should be ",(0,r.jsx)(o.strong,{children:"idempotent"}),", meaning it should be safe to retry them multiple times.\nIf a workflow fails while executing a step, it retries the step during recovery.\nHowever, once a step completes and is checkpointed, it is never re-executed."]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(o.h2,{id:"application-and-workflow-versions",children:"Application and Workflow Versions"}),"\n",(0,r.jsxs)(o.p,{children:["If code changes between when a workflow starts and when it its recovered, safe recovery may not be possible.\nTo guard against this, DBOS ",(0,r.jsx)(o.em,{children:"versions"})," applications and their workflows.\nWhen DBOS is launched, it computes an application version from a hash of the source code of your workflows. You can override the version through configuration.\nAll workflows are tagged with the application version on which they started."]}),"\n",(0,r.jsxs)(o.p,{children:["When DBOS tries to recover workflows, it only recovers workflows whose version matches the current application version.\nThis prevents unsafe recovery of workflows that depend on different code.\nTo safely recover workflows started on an older version of your code, you should start a process running that code version.\nAlternatively, you can use the ",(0,r.jsx)(o.a,{href:"/production/self-hosting/workflow-management#forking-workflows",children:"workflow fork"})," operation to restart a workflow from a specific step on a specific code version.\nFor more information, see the ",(0,r.jsx)(o.a,{href:"/production/self-hosting/workflow-recovery",children:"workflow recovery documentation"}),"."]}),"\n",(0,r.jsx)(o.h2,{id:"durable-queues",children:"Durable Queues"}),"\n",(0,r.jsxs)(o.p,{children:["One powerful feature of DBOS is that you can ",(0,r.jsx)(o.strong,{children:"enqueue"})," workflows for distributed execution with flow control.\nYou can enqueue a workflow from within a DBOS app directly or from anywhere using a DBOS client."]}),"\n",(0,r.jsx)(o.p,{children:"An enqueued workflow may be dequeued and executed by any of your application's servers.\nAll processes running DBOS periodically poll their queues to find and execute new work.\nEssentially, all of your application servers act as queue workers, as in this diagram:"}),"\n",(0,r.jsx)("img",{src:t(5717).A,alt:"DBOS Queues",width:"750",className:"custom-img"}),"\n",(0,r.jsxs)(o.p,{children:["Sometimes, you want to separate the worker servers that execute your queued tasks from the rest of your application.\nFor example, you may want to scale them separately.\nTo do this in DBOS, deploy your queue workers as a separate ",(0,r.jsx)(o.a,{href:"#using-dbos-in-a-distributed-setting",children:"application"})," with their own system database.\nThen, use the DBOS Client (",(0,r.jsx)(o.a,{href:"/python/reference/client",children:"Python"}),", ",(0,r.jsx)(o.a,{href:"/typescript/reference/client",children:"TypeScript"}),", ",(0,r.jsx)(o.a,{href:"/golang/reference/client",children:"Go"}),") to enqueue and manage workflows on your worker application from your other applications."]}),"\n",(0,r.jsxs)(o.p,{children:["To help you operate at scale, DBOS queues provide ",(0,r.jsx)(o.strong,{children:"flow control"}),".\nYou can customize the rate and concurrency at which workflows are dequeued and executed.\nFor example, you can set a ",(0,r.jsx)(o.strong,{children:"worker concurrency"})," for each of your queues on each of your servers, limiting how many workflows from that queue may execute concurrently on that server.\nFor more information on queues, see the docs (",(0,r.jsx)(o.a,{href:"/python/tutorials/queue-tutorial",children:"Python"}),", ",(0,r.jsx)(o.a,{href:"/typescript/tutorials/queue-tutorial",children:"TypeScript"}),", ",(0,r.jsx)(o.a,{href:"/golang/tutorials/queue-tutorial",children:"Go"}),", ",(0,r.jsx)(o.a,{href:"/java/tutorials/queue-tutorial",children:"Java"}),")."]}),"\n",(0,r.jsx)(o.h2,{id:"self-hosting-dbos-with-conductor",children:"Self-Hosting DBOS with Conductor"}),"\n",(0,r.jsx)(o.p,{children:"The simplest way to operate DBOS durable workflows in production is to connect your application to Conductor.\nConductor is an optional management service that helps you self-host DBOS applications.\nIt provides:"}),"\n",(0,r.jsxs)(o.ul,{children:["\n",(0,r.jsxs)(o.li,{children:[(0,r.jsx)(o.a,{href:"/production/self-hosting/workflow-recovery",children:(0,r.jsx)(o.strong,{children:"Distributed workflow recovery"})}),": In a distributed environment with many executors running durable workflows, Conductor automatically detects when the execution of a durable workflow is interrupted (for example, if its executor is restarted, interrupted, or crashes) and recovers the workflow to another healthy executor."]}),"\n",(0,r.jsxs)(o.li,{children:[(0,r.jsx)(o.a,{href:"/production/self-hosting/workflow-management",children:(0,r.jsx)(o.strong,{children:"Workflow and queue observability"})}),": Conductor provides dashboards of all active and past workflows and all queued tasks, including their status, inputs, outputs, and steps."]}),"\n",(0,r.jsxs)(o.li,{children:[(0,r.jsx)(o.a,{href:"/production/self-hosting/workflow-management",children:(0,r.jsx)(o.strong,{children:"Workflow and queue management"})}),": From the Conductor dashboard, cancel, resume, or restart any workflow execution and manage the tasks in your distributed queues."]}),"\n"]}),"\n",(0,r.jsx)(o.p,{children:"Architecturally, Conductor looks like this:"}),"\n",(0,r.jsx)("img",{src:t(5284).A,alt:"DBOS Conductor Architecture",width:"750",className:"custom-img"}),"\n",(0,r.jsx)(o.p,{children:"Each of your application servers opens a secure websocket connection to Conductor.\nAll of Conductor's features are powered by these websocket connections.\nWhen you open a Conductor dashboard in your browser, your request is sent over websocket to one of your application servers, which serves the request (for example, retrieving a list of recent workflows) and sends the result back through the websocket.\nIf one of your application servers fails, Conductor detects the failure through the closed websocket connection and, after a grace period, directs another server to recover its workflows.\nThis architecture has two useful implications:"}),"\n",(0,r.jsxs)(o.ol,{children:["\n",(0,r.jsxs)(o.li,{children:["Conductor is ",(0,r.jsx)(o.strong,{children:"secure"})," and ",(0,r.jsx)(o.strong,{children:"privacy-preserving"}),". It does not have access to your database, nor does it need direct access to your application servers. Instead, your servers open outbound websocket connections to it and communicate exclusively through its websocket protocol."]}),"\n",(0,r.jsxs)(o.li,{children:["Conductor is ",(0,r.jsx)(o.strong,{children:"out of band"})," and ",(0,r.jsx)(o.strong,{children:"off your critical path"}),". Conductor is ",(0,r.jsx)(o.strong,{children:"only"})," used for observability and recovery and is never involved in workflow execution (unlike the external orchestrators of other workflow systems).\nIf your application's connection to Conductor is interrupted, it will continue to operate normally, and any failed workflows will automatically be recovered as soon as the connection is restored."]}),"\n"]}),"\n",(0,r.jsxs)(o.p,{children:["For more information on Conductor, see ",(0,r.jsx)(o.a,{href:"/production/self-hosting/conductor",children:"its docs"}),"."]}),"\n",(0,r.jsx)(o.h2,{id:"host-applications-on-dbos-cloud",children:"Host Applications on DBOS Cloud"}),"\n",(0,r.jsx)(o.p,{children:"You can deploy DBOS applications to DBOS Cloud.\nDBOS Cloud is a serverless platform for durably executed applications.\nIt provides:"}),"\n",(0,r.jsxs)(o.ul,{children:["\n",(0,r.jsxs)(o.li,{children:[(0,r.jsx)(o.a,{href:"/production/dbos-cloud/application-management",children:(0,r.jsx)(o.strong,{children:"Application hosting and autoscaling"})}),": Managed hosting of your application in the cloud, automatically scaling to millions of users. Applications are charged only for the CPU time they actually consume."]}),"\n",(0,r.jsxs)(o.li,{children:[(0,r.jsx)(o.a,{href:"/production/dbos-cloud/application-management",children:(0,r.jsx)(o.strong,{children:"Automatic workflow version management"})}),": DBOS Cloud seamlessly manages code version upgrades, launching new workflows on new code versions while completing old workflows on old code versions."]}),"\n",(0,r.jsxs)(o.li,{children:[(0,r.jsx)(o.a,{href:"/production/dbos-cloud/application-management",children:(0,r.jsx)(o.strong,{children:"Managed workflow recovery"})}),": If a cloud executor is interrupted, crashed, or restarted, each of its workflows is automatically recovered by another executor."]}),"\n",(0,r.jsxs)(o.li,{children:[(0,r.jsx)(o.a,{href:"/production/dbos-cloud/workflow-management",children:(0,r.jsx)(o.strong,{children:"Workflow and queue observability"})}),": Dashboards of all active and past workflows and all queued tasks, including their status, inputs, outputs, and steps."]}),"\n",(0,r.jsxs)(o.li,{children:[(0,r.jsx)(o.a,{href:"/production/dbos-cloud/workflow-management",children:(0,r.jsx)(o.strong,{children:"Workflow and queue management"})}),": From an online dashboard, cancel, resume, or restart any workflow execution and manage the tasks in your distributed queues."]}),"\n"]}),"\n",(0,r.jsxs)(o.p,{children:["See ",(0,r.jsx)(o.a,{href:"/production/dbos-cloud/deploying-to-cloud",children:(0,r.jsx)(o.strong,{children:"Deploying to DBOS Cloud"})})," to learn more."]})]})}function u(e={}){const{wrapper:o}={...(0,n.R)(),...e.components};return o?(0,r.jsx)(o,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}},4337:(e,o,t)=>{t.d(o,{A:()=>s});const s=t.p+"assets/images/api-worker-4886260a57fb925ca48bd183e05ebe89.png"},9452:(e,o,t)=>{t.d(o,{A:()=>s});const s=t.p+"assets/images/dbos-architecture-be89e880bc75334f1743bf8c256209cf.png"},5284:(e,o,t)=>{t.d(o,{A:()=>s});const s=t.p+"assets/images/dbos-conductor-architecture-052b057ba4714b1909a6dd3bc4a95f9e.png"},5717:(e,o,t)=>{t.d(o,{A:()=>s});const s=t.p+"assets/images/dbos-queues-3c55540c18343e0602412501556bbc40.png"},8453:(e,o,t)=>{t.d(o,{R:()=>i,x:()=>a});var s=t(6540);const r={},n=s.createContext(r);function i(e){const o=s.useContext(n);return s.useMemo((function(){return"function"==typeof e?e(o):{...o,...e}}),[o,e])}function a(e){let o;return o=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:i(e.components),s.createElement(n.Provider,{value:o},e.children)}}}]);