"use strict";(self.webpackChunkdbos_docs=self.webpackChunkdbos_docs||[]).push([[9102],{2897:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>p,frontMatter:()=>a,metadata:()=>s,toc:()=>d});const s=JSON.parse('{"id":"python/examples/document-detective","title":"Document Ingestion Pipeline","description":"In this example, we\'ll use DBOS to build a reliable and scalable data processing pipeline.","source":"@site/docs/python/examples/document-detective.md","sourceDirName":"python/examples","slug":"/python/examples/document-detective","permalink":"/python/examples/document-detective","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"displayed_sidebar":"examplesSidebar","sidebar_position":2,"title":"Document Ingestion Pipeline"},"sidebar":"examplesSidebar","previous":{"title":"Fault-Tolerant Checkout","permalink":"/python/examples/widget-store"},"next":{"title":"Stock Tracker","permalink":"/python/examples/stock-tracker"}}');var i=t(4848),o=t(8453);const a={displayed_sidebar:"examplesSidebar",sidebar_position:2,title:"Document Ingestion Pipeline"},r=void 0,l={},d=[{value:"Import and Initialize the App",id:"import-and-initialize-the-app",level:2},{value:"Building a Durable Data Ingestion Pipeline",id:"building-a-durable-data-ingestion-pipeline",level:2},{value:"Chatting With Your Data",id:"chatting-with-your-data",level:2},{value:"Try it Yourself!",id:"try-it-yourself",level:2},{value:"Creating an OpenAI Account",id:"creating-an-openai-account",level:3},{value:"Deploying to the Cloud",id:"deploying-to-the-cloud",level:3},{value:"Running Locally",id:"running-locally",level:3},{value:"Indexing Documents",id:"indexing-documents",level:3}];function c(e){const n={a:"a",code:"code",em:"em",h2:"h2",h3:"h3",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",...(0,o.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsxs)(n.p,{children:["In this example, we'll use DBOS to build a ",(0,i.jsx)(n.strong,{children:"reliable and scalable data processing pipeline"}),".\nWe'll show how DBOS can help you process many items concurrently and seamlessly recover from failures.\nSpecifically, we'll build a pipeline that indexes PDF documents for RAG, though you can use a similar design pattern to build almost any data pipeline."]}),"\n",(0,i.jsx)(n.p,{children:"To show the pipeline works, we'll also build a chat agent that can accurately answer questions about the indexed documents.\nFor example, here's what the chat agent looks like after ingesting the last three years of Apple 10-K filings.\nIt can accurately answer questions about Apple's financials:"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"Document Detective UI",src:t(6341).A+"",width:"914",height:"443"})}),"\n",(0,i.jsxs)(n.p,{children:["All source code is ",(0,i.jsx)(n.a,{href:"https://github.com/dbos-inc/dbos-demo-apps/tree/main/python/document-detective",children:"available on GitHub"}),"."]}),"\n",(0,i.jsx)(n.h2,{id:"import-and-initialize-the-app",children:"Import and Initialize the App"}),"\n",(0,i.jsx)(n.p,{children:"Let's start off with imports and initializing DBOS."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import os\nfrom tempfile import TemporaryDirectory\nfrom typing import List\n\nimport requests\nfrom dbos import DBOS, DBOSConfig, Queue, WorkflowHandle\nfrom fastapi import FastAPI\nfrom fastapi.responses import HTMLResponse\nfrom llama_index.core import Settings, StorageContext, VectorStoreIndex\nfrom llama_index.readers.file import PDFReader\nfrom llama_index.vector_stores.postgres import PGVectorStore\nfrom pydantic import BaseModel, HttpUrl\n\nfrom .schema import chat_history\n\napp = FastAPI()\nconfig: DBOSConfig = {\n    "name": "document-detective",\n    "database_url": os.environ.get("DBOS_DATABASE_URL"),\n}\nDBOS(fastapi=app, config=config)\n'})}),"\n",(0,i.jsx)(n.p,{children:"Next, let's initialize LlamaIndex to store and query the vector index we'll be constructing."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'def configure_index():\n    Settings.chunk_size = 512\n    dbos_config = DBOS.config\n    db = dbos_config["database"]\n    vector_store = PGVectorStore.from_params(\n        database=db["app_db_name"],\n        host=db["hostname"],\n        password=db["password"],\n        port=db["port"],\n        user=db["username"],\n        perform_setup=False,\n    )\n    storage_context = StorageContext.from_defaults(vector_store=vector_store)\n    index = VectorStoreIndex([], storage_context=storage_context)\n    chat_engine = index.as_chat_engine()\n    return index, chat_engine\n\n\nindex, chat_engine = configure_index()\n'})}),"\n",(0,i.jsx)(n.h2,{id:"building-a-durable-data-ingestion-pipeline",children:"Building a Durable Data Ingestion Pipeline"}),"\n",(0,i.jsxs)(n.p,{children:["Now, let's write the document ingestion pipeline.\nBecause ingesting and indexing documents may take a long time, we need to build a pipeline that's both ",(0,i.jsx)(n.em,{children:"concurrent"})," and ",(0,i.jsx)(n.em,{children:"reliable"}),".\nIt needs to process multiple documents at once and it needs to be resilient to failures, so if the application is interrupted or restarted, or encounters an error, it can recover from where it left off instead of restarting from the beginning or losing some documents entirely."]}),"\n",(0,i.jsxs)(n.p,{children:["We'll build a concurrent, reliable data ingestion pipeline using DBOS ",(0,i.jsx)(n.a,{href:"/python/tutorials/queue-tutorial",children:"queues"})," and ",(0,i.jsx)(n.a,{href:"/python/tutorials/workflow-tutorial",children:"durable execution"}),".\nThis workflow takes in a batch of document URLs and enqueues them for ingestion.\nIt then waits for them all to complete and counts how many total documents and pages were ingested.\nIf it's ever interrupted or restarted, it recovers the ingestion of each document from the last completed step, guaranteeing that every document is ingested and none are lost."]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'queue = Queue("indexing_queue")\n\n\n@DBOS.workflow()\ndef indexing_workflow(urls: List[HttpUrl]):\n    handles: List[WorkflowHandle] = []\n    for url in urls:\n        handle = queue.enqueue(index_document, url)\n        handles.append(handle)\n    indexed_pages = 0\n    for handle in handles:\n        indexed_pages += handle.get_result()\n    DBOS.logger.info(f"Indexed {len(urls)} documents totaling {indexed_pages} pages")\n'})}),"\n",(0,i.jsx)(n.p,{children:"Now, let's write the key step of this pipeline: the function that ingests a PDF document from a URL.\nThis function downloads a document, scans it into pages, then uses LlamaIndex to embed it and store the embedding in Postgres."}),"\n",(0,i.jsxs)(n.p,{children:["Because this entire procedure is implemented in a single function, we annotate it with ",(0,i.jsx)(n.a,{href:"/python/tutorials/step-tutorial",children:(0,i.jsx)(n.code,{children:"@DBOS.step()"})})," to mark it as a ",(0,i.jsx)(n.em,{children:"step"})," of the indexing workflow.\nAdditionally, in case of transient failures (for example in downloading the document) we set it to automatically retry up to 5 times with exponential backoff.\nIn a more complex pipeline, we might process each document with a durable workflow of several steps; this would be a ",(0,i.jsx)(n.em,{children:"child workflow"})," of the indexing workflow."]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'@DBOS.step(retries_allowed=True, max_attempts=5)\ndef index_document(document_url: HttpUrl) -> int:\n    with TemporaryDirectory() as temp_dir:\n        temp_file_path = os.path.join(temp_dir, "file.pdf")\n        with open(temp_file_path, "wb") as temp_file:\n            with requests.get(document_url, stream=True) as r:\n                r.raise_for_status()\n                for page in r.iter_content(chunk_size=8192):\n                    temp_file.write(page)\n            temp_file.seek(0)\n            reader = PDFReader()\n            pages = reader.load_data(temp_file_path)\n    for page in pages:\n        index.insert(page)\n    return len(pages)\n'})}),"\n",(0,i.jsx)(n.p,{children:"Next, let's write the endpoint for indexing.\nIt starts the indexing workflow in the background on a batch of documents."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'class URLList(BaseModel):\n    urls: List[HttpUrl]\n\n\n@app.post("/index")\nasync def index_endpoint(urls: URLList):\n    DBOS.start_workflow(indexing_workflow, urls.urls)\n'})}),"\n",(0,i.jsx)(n.h2,{id:"chatting-with-your-data",children:"Chatting With Your Data"}),"\n",(0,i.jsx)(n.p,{children:"Now, let's build the backend for a chatbot agent you can use to ask questions about the documents you've ingested."}),"\n",(0,i.jsx)(n.p,{children:"Each time we get a chat message, we call this workflow with three steps:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"Store the incoming chat message in Postgres."}),"\n",(0,i.jsx)(n.li,{children:"Query LlamaIndex to respond to the message using RAG."}),"\n",(0,i.jsx)(n.li,{children:"Store the response in Postgres."}),"\n"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'class ChatSchema(BaseModel):\n    message: str\n\n\n@app.post("/chat")\n@DBOS.workflow()\ndef chat_workflow(chat: ChatSchema):\n    insert_chat(chat.message, True)\n    response = query_model(chat.message)\n    insert_chat(response, False)\n    return {"content": response, "isUser": True}\n\n\n@DBOS.transaction()\ndef insert_chat(content: str, is_user: bool):\n    DBOS.sql_session.execute(\n        chat_history.insert().values(content=content, is_user=is_user)\n    )\n\n\n@DBOS.step()\ndef query_model(message: str) -> str:\n    return str(chat_engine.chat(message))\n'})}),"\n",(0,i.jsx)(n.p,{children:"Let's also write a history endpoint that retrieves all past chats from the database."}),"\n",(0,i.jsx)(n.p,{children:"This function is called when we open up the chatbot so it can display your chat history."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'@app.get("/history")\ndef history_endpoint():\n    return get_chats()\n\n\n@DBOS.transaction()\ndef get_chats():\n    stmt = chat_history.select().order_by(chat_history.c.created_at.asc())\n    result = DBOS.sql_session.execute(stmt)\n    return [{"content": row.content, "isUser": row.is_user} for row in result]\n'})}),"\n",(0,i.jsx)(n.p,{children:"Finally, let's serve the app's frontend from an HTML file using FastAPI.\nIn production, we recommend using DBOS primarily for the backend, with your frontend deployed elsewhere."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'@app.get("/")\ndef frontend():\n    with open(os.path.join("html", "app.html")) as file:\n        html = file.read()\n    return HTMLResponse(html)\n'})}),"\n",(0,i.jsx)(n.h2,{id:"try-it-yourself",children:"Try it Yourself!"}),"\n",(0,i.jsx)(n.h3,{id:"creating-an-openai-account",children:"Creating an OpenAI Account"}),"\n",(0,i.jsxs)(n.p,{children:["To run this app, you need an OpenAI developer account.\nObtain an API key ",(0,i.jsx)(n.a,{href:"https://platform.openai.com/api-keys",children:"here"})," and set up a payment method for your account ",(0,i.jsx)(n.a,{href:"https://platform.openai.com/account/billing/overview",children:"here"}),".\nThis bot uses ",(0,i.jsx)(n.code,{children:"gpt-3.5-turbo"})," for text generation.\nMake sure you have some credits (~$1) to use it."]}),"\n",(0,i.jsx)(n.p,{children:"Set your API key as an environment variable:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-shell",children:"export OPENAI_API_KEY=<your_openai_key>\n"})}),"\n",(0,i.jsx)(n.h3,{id:"deploying-to-the-cloud",children:"Deploying to the Cloud"}),"\n",(0,i.jsx)(n.p,{children:"To deploy this app to DBOS Cloud, first install the DBOS Cloud CLI (requires Node):"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-shell",children:"npm i -g @dbos-inc/dbos-cloud\n"})}),"\n",(0,i.jsxs)(n.p,{children:["Then clone the ",(0,i.jsx)(n.a,{href:"https://github.com/dbos-inc/dbos-demo-apps",children:"dbos-demo-apps"})," repository and deploy:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-shell",children:"git clone https://github.com/dbos-inc/dbos-demo-apps.git\ncd python/document-detective\ndbos-cloud app deploy\n"})}),"\n",(0,i.jsxs)(n.p,{children:["This command outputs a URL\u2014visit it to see your chat agent!\nYou can also visit the ",(0,i.jsx)(n.a,{href:"https://console.dbos.dev/login-redirect",children:"DBOS Cloud Console"})," to see your app's status and logs."]}),"\n",(0,i.jsx)(n.h3,{id:"running-locally",children:"Running Locally"}),"\n",(0,i.jsxs)(n.p,{children:["First, clone and enter the ",(0,i.jsx)(n.a,{href:"https://github.com/dbos-inc/dbos-demo-apps",children:"dbos-demo-apps"})," repository:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-shell",children:"git clone https://github.com/dbos-inc/dbos-demo-apps.git\ncd python/document-detective\n"})}),"\n",(0,i.jsx)(n.p,{children:"Then create a virtual environment:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-shell",children:"python3 -m venv .venv\nsource .venv/bin/activate\n"})}),"\n",(0,i.jsx)(n.p,{children:"Then start your app:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-shell",children:"pip install -r requirements.txt\ndbos migrate\ndbos start\n"})}),"\n",(0,i.jsxs)(n.p,{children:["Visit ",(0,i.jsx)(n.a,{href:"http://localhost:8000",children:(0,i.jsx)(n.code,{children:"http://localhost:8000"})})," to see your chat agent!"]}),"\n",(0,i.jsx)(n.h3,{id:"indexing-documents",children:"Indexing Documents"}),"\n",(0,i.jsxs)(n.p,{children:["To index a batch of PDF documents, send a list of their URLs in a POST request to the ",(0,i.jsx)(n.code,{children:"/index"})," endpoint."]}),"\n",(0,i.jsxs)(n.p,{children:["For example, try this cURL command to index Apple's SEC 10-K filings for 2021, 2022, and 2023\nThe application URL you should use is ",(0,i.jsx)(n.code,{children:"http://localhost:8000"})," locally and your app URL in DBOS Cloud:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-shell",children:'curl -X POST "http://<URL>/index" \\\n     -H "Content-Type: application/json" \\\n     -d \'{"urls": ["https://d18rn0p25nwr6d.cloudfront.net/CIK-0000320193/faab4555-c69b-438a-aaf7-e09305f87ca3.pdf", "https://d18rn0p25nwr6d.cloudfront.net/CIK-0000320193/b4266e40-1de6-4a34-9dfb-8632b8bd57e0.pdf", "https://d18rn0p25nwr6d.cloudfront.net/CIK-0000320193/42ede86f-6518-450f-bc88-60211bf39c6d.pdf"]}\'\n\n'})})]})}function p(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(c,{...e})}):c(e)}},6341:(e,n,t)=>{t.d(n,{A:()=>s});const s=t.p+"assets/images/document_detective-d5318965aecd181334392e0ea0e9d772.png"},8453:(e,n,t)=>{t.d(n,{R:()=>a,x:()=>r});var s=t(6540);const i={},o=s.createContext(i);function a(e){const n=s.useContext(o);return s.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:a(e.components),s.createElement(o.Provider,{value:n},e.children)}}}]);