"use strict";(self.webpackChunkdbos_docs=self.webpackChunkdbos_docs||[]).push([[9102],{9073:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>d,contentTitle:()=>r,default:()=>p,frontMatter:()=>o,metadata:()=>i,toc:()=>l});const i=JSON.parse('{"id":"python/examples/document-detective","title":"Document Ingestion Pipeline","description":"In this example, we\'ll use DBOS to build a reliable and scalable data processing pipeline.","source":"@site/docs/python/examples/document-detective.md","sourceDirName":"python/examples","slug":"/python/examples/document-detective","permalink":"/python/examples/document-detective","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":20,"frontMatter":{"displayed_sidebar":"examplesSidebar","sidebar_position":20,"title":"Document Ingestion Pipeline"},"sidebar":"examplesSidebar","previous":{"title":"Hacker News Research Agent","permalink":"/python/examples/hacker-news-agent"},"next":{"title":"Fault-Tolerant Checkout","permalink":"/python/examples/widget-store"}}');var a=t(4848),s=t(8453);const o={displayed_sidebar:"examplesSidebar",sidebar_position:20,title:"Document Ingestion Pipeline"},r=void 0,d={},l=[{value:"Import and Initialize the App",id:"import-and-initialize-the-app",level:2},{value:"Building a Durable Data Ingestion Pipeline",id:"building-a-durable-data-ingestion-pipeline",level:2},{value:"Chatting With Your Data",id:"chatting-with-your-data",level:2},{value:"Try it Yourself!",id:"try-it-yourself",level:2}];function c(e){const n={a:"a",code:"code",em:"em",h2:"h2",img:"img",p:"p",pre:"pre",strong:"strong",...(0,s.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsxs)(n.p,{children:["In this example, we'll use DBOS to build a ",(0,a.jsx)(n.strong,{children:"reliable and scalable data processing pipeline"}),".\nWe'll show how DBOS can help you horizontally scale an application to process many items concurrently and seamlessly recover from failures.\nSpecifically, we'll build a pipeline that indexes PDF documents for RAG, though you can use a similar design pattern to build almost any data pipeline."]}),"\n",(0,a.jsx)(n.p,{children:"To show the pipeline works, we'll also build a simple chat agent that can accurately answer questions about the indexed documents.\nFor example, after ingesting the last few years of Apple 10-K filings, the chat agent can accurately answer questions about Apple's financials:"}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.img,{alt:"Document Detective UI",src:t(8722).A+"",width:"914",height:"443"})}),"\n",(0,a.jsxs)(n.p,{children:["All source code is ",(0,a.jsx)(n.a,{href:"https://github.com/dbos-inc/dbos-demo-apps/tree/main/python/document-detective",children:"available on GitHub"}),"."]}),"\n",(0,a.jsx)(n.h2,{id:"import-and-initialize-the-app",children:"Import and Initialize the App"}),"\n",(0,a.jsx)(n.p,{children:"Let's start with imports and initializing DBOS."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import os\nfrom tempfile import TemporaryDirectory\nfrom typing import List\n\nimport requests\nimport uvicorn\nfrom dbos import DBOS, DBOSConfig, Queue, WorkflowHandle\nfrom fastapi import FastAPI\nfrom fastapi.responses import HTMLResponse\nfrom llama_index.core import Document, Settings, StorageContext, VectorStoreIndex\nfrom llama_index.readers.file import PDFReader\nfrom llama_index.vector_stores.postgres import PGVectorStore\nfrom pydantic import BaseModel, HttpUrl\nfrom sqlalchemy import make_url\n\ndatabase_url = os.environ.get("DBOS_SYSTEM_DATABASE_URL")\nif not database_url:\n    raise Exception("DBOS_SYSTEM_DATABASE_URL not set")\n\napp = FastAPI()\nconfig: DBOSConfig = {\n    "name": "document-detective",\n    "system_database_url": database_url,\n    "conductor_key": os.environ.get("CONDUCTOR_KEY"),\n}\nDBOS(config=config)\n'})}),"\n",(0,a.jsx)(n.p,{children:"Next, let's initialize LlamaIndex to store and query the vector index we'll be constructing."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"def configure_index():\n    Settings.chunk_size = 512\n    db = make_url(database_url)\n    vector_store = PGVectorStore.from_params(\n        database=db.database,\n        host=db.host,\n        password=db.password,\n        port=db.port,\n        user=db.username,\n        perform_setup=False,  # Set up during migration step\n    )\n    storage_context = StorageContext.from_defaults(vector_store=vector_store)\n    index = VectorStoreIndex([], storage_context=storage_context)\n    chat_engine = index.as_chat_engine()\n    return index, chat_engine\n\n\nindex, chat_engine = configure_index()\n"})}),"\n",(0,a.jsx)(n.h2,{id:"building-a-durable-data-ingestion-pipeline",children:"Building a Durable Data Ingestion Pipeline"}),"\n",(0,a.jsxs)(n.p,{children:["Now, let's write the document ingestion pipeline.\nBecause ingesting and indexing documents may take a long time, we need to build a pipeline that's both ",(0,a.jsx)(n.em,{children:"concurrent"})," and ",(0,a.jsx)(n.em,{children:"reliable"}),".\nIt needs to process multiple documents at once and it needs to be resilient to failures, so if the application is interrupted or restarted, or encounters an error, it can recover from where it left off instead of restarting from the beginning or losing some documents entirely."]}),"\n",(0,a.jsx)(n.p,{children:"We'll build a concurrent, reliable data ingestion pipeline using DBOS workflows and queues.\nThis workflow takes in a batch of document URLs, enqueues them all for indexing, and waits for all documents to finish being indexed.\nIf it's ever interrupted or restarted, it recovers the indexing of each document from the last completed step, guaranteeing that every document is indexed and none are lost."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'queue = Queue("indexing_queue")\n\n@DBOS.workflow()\ndef index_documents(urls: List[HttpUrl]):\n    handles: List[WorkflowHandle] = []\n    # Enqueue each document for indexing\n    for url in urls:\n        handle = queue.enqueue(index_document, url)\n        handles.append(handle)\n    # Wait for all documents to finish indexing, count the total number of indexed pages\n    indexed_pages = 0\n    for handle in handles:\n        indexed_pages += handle.get_result()\n    print(f"Indexed {len(urls)} documents totaling {indexed_pages} pages")\n'})}),"\n",(0,a.jsxs)(n.p,{children:["This workflow indexes an individual document.\nIt calls two steps: ",(0,a.jsx)(n.code,{children:"download_document"})," to download the document and parse it into pages, then ",(0,a.jsx)(n.code,{children:"index_page"})," to add a parsed page to the vector index."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"@DBOS.workflow()\ndef index_document(document_url: HttpUrl) -> int:\n    pages = download_document(document_url)\n    for page in pages:\n        index_page(page)\n    return len(pages)\n"})}),"\n",(0,a.jsxs)(n.p,{children:["Here's the code for the steps in ",(0,a.jsx)(n.code,{children:"index_document"}),":"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'@DBOS.step()\ndef download_document(document_url: HttpUrl):\n    # Download the document to a temporary file\n    print(f"Downloading document from {document_url}")\n    with TemporaryDirectory() as temp_dir:\n        temp_file_path = os.path.join(temp_dir, "document.pdf")\n        with open(temp_file_path, "wb") as temp_file:\n            with requests.get(document_url, stream=True) as r:\n                r.raise_for_status()\n                for page in r.iter_content(chunk_size=8192):\n                    temp_file.write(page)\n        # Parse the document into pages\n        reader = PDFReader()\n        pages = reader.load_data(temp_file_path)\n    return pages\n\n\n@DBOS.step()\ndef index_page(page: Document):\n    # Insert a page into the vector index\n    try:\n        index.insert(page)\n    except Exception as e:\n        print("Error indexing page:", page, e)\n'})}),"\n",(0,a.jsx)(n.p,{children:"Next, let's write the endpoint for indexing.\nIt starts the indexing workflow in the background on a batch of documents."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'class URLList(BaseModel):\n    urls: List[HttpUrl]\n\n@app.post("/index")\ndef index_endpoint(urls: URLList):\n    DBOS.start_workflow(index_documents, urls.urls)\n'})}),"\n",(0,a.jsx)(n.h2,{id:"chatting-with-your-data",children:"Chatting With Your Data"}),"\n",(0,a.jsx)(n.p,{children:"Now, let's write a simple in-memory chat agent so we can query our data.\nEvery time it gets a question, it answers using a RAG chat engine powered by the vector index."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'class ChatSchema(BaseModel):\n    message: str\n\nchat_history = []\n\n@app.post("/chat")\ndef chat(chat: ChatSchema):\n    query = {"content": chat.message, "isUser": False}\n    chat_history.append(query)\n    responseMessage = str(chat_engine.chat(chat.message))\n    response = {"content": responseMessage, "isUser": True}\n    chat_history.append(response)\n    return response\n\n@app.get("/history")\ndef history_endpoint():\n    return chat_history\n'})}),"\n",(0,a.jsx)(n.p,{children:"We'll serve the app's frontend from an HTML file using FastAPI."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'@app.get("/")\ndef frontend():\n    with open(os.path.join("html", "app.html")) as file:\n        html = file.read()\n    return HTMLResponse(html)\n'})}),"\n",(0,a.jsx)(n.p,{children:"Finally, let's write a main function to launch DBOS and start our app:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'if __name__ == "__main__":\n    DBOS.launch()\n    uvicorn.run(app, host="0.0.0.0", port=8000)\n'})}),"\n",(0,a.jsx)(n.h2,{id:"try-it-yourself",children:"Try it Yourself!"}),"\n",(0,a.jsxs)(n.p,{children:["Clone and enter the ",(0,a.jsx)(n.a,{href:"https://github.com/dbos-inc/dbos-demo-apps",children:"dbos-demo-apps"})," repository:"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-shell",children:"git clone https://github.com/dbos-inc/dbos-demo-apps.git\ncd python/document-detective\n"})}),"\n",(0,a.jsxs)(n.p,{children:["Then follow the instructions in the ",(0,a.jsx)(n.a,{href:"https://github.com/dbos-inc/dbos-demo-apps/tree/main/python/document-detective",children:"README"})," to run the app."]})]})}function p(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(c,{...e})}):c(e)}},8722:(e,n,t)=>{t.d(n,{A:()=>i});const i=t.p+"assets/images/document_detective-d5318965aecd181334392e0ea0e9d772.png"},8453:(e,n,t)=>{t.d(n,{R:()=>o,x:()=>r});var i=t(6540);const a={},s=i.createContext(a);function o(e){const n=i.useContext(s);return i.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:o(e.components),i.createElement(s.Provider,{value:n},e.children)}}}]);