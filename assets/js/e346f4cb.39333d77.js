"use strict";(self.webpackChunkdbos_docs=self.webpackChunkdbos_docs||[]).push([[6388],{4633:(e,n,o)=>{o.r(n),o.d(n,{assets:()=>a,contentTitle:()=>u,default:()=>d,frontMatter:()=>s,metadata:()=>t,toc:()=>l});const t=JSON.parse('{"id":"python/tutorials/queue-tutorial","title":"Queues & Concurrency","description":"You can use queues to run many workflows at once with managed concurrency.","source":"@site/docs/python/tutorials/queue-tutorial.md","sourceDirName":"python/tutorials","slug":"/python/tutorials/queue-tutorial","permalink":"/python/tutorials/queue-tutorial","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":40,"frontMatter":{"sidebar_position":40,"title":"Queues & Concurrency","toc_max_heading_level":3},"sidebar":"tutorialSidebar","previous":{"title":"Steps","permalink":"/python/tutorials/step-tutorial"},"next":{"title":"Communicating with Workflows","permalink":"/python/tutorials/workflow-communication"}}');var r=o(4848),i=o(8453);const s={sidebar_position:40,title:"Queues & Concurrency",toc_max_heading_level:3},u=void 0,a={},l=[{value:"Queue Example",id:"queue-example",level:3},{value:"Enqueueing from Another Application",id:"enqueueing-from-another-application",level:3},{value:"Managing Concurrency",id:"managing-concurrency",level:3},{value:"Worker Concurrency",id:"worker-concurrency",level:4},{value:"Global Concurrency",id:"global-concurrency",level:4},{value:"In-Order Processing",id:"in-order-processing",level:4},{value:"Rate Limiting",id:"rate-limiting",level:3},{value:"Setting Timeouts",id:"setting-timeouts",level:2},{value:"Partitioning Queues",id:"partitioning-queues",level:2},{value:"Deduplication",id:"deduplication",level:2},{value:"Priority",id:"priority",level:2}];function c(e){const n={a:"a",admonition:"admonition",code:"code",em:"em",h2:"h2",h3:"h3",h4:"h4",p:"p",pre:"pre",strong:"strong",...(0,i.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsxs)(n.p,{children:["You can use queues to run many workflows at once with managed concurrency.\nQueues provide ",(0,r.jsx)(n.em,{children:"flow control"}),", letting you manage how many workflows run at once or how often workflows are started."]}),"\n",(0,r.jsx)(n.p,{children:"To create a queue, specify its name:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from dbos import Queue\n\nqueue = Queue("example_queue")\n'})}),"\n",(0,r.jsxs)(n.p,{children:["You can then enqueue any DBOS workflow or step.\nEnqueuing a function submits it for execution and returns a ",(0,r.jsx)(n.a,{href:"/python/reference/workflow_handles",children:"handle"})," to it.\nQueued tasks are started in first-in, first-out (FIFO) order."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'queue = Queue("example_queue")\n\n@DBOS.workflow()\ndef process_task(task):\n  ...\n\ntask = ...\nhandle = queue.enqueue(process_task, task)\n'})}),"\n",(0,r.jsx)(n.h3,{id:"queue-example",children:"Queue Example"}),"\n",(0,r.jsx)(n.p,{children:"Here's an example of a workflow using a queue to process tasks concurrently:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from dbos import DBOS, Queue\n\nqueue = Queue("example_queue")\n\n@DBOS.workflow()\ndef process_task(task):\n  ...\n\n@DBOS.workflow()\ndef process_tasks(tasks):\n  task_handles = []\n  # Enqueue each task so all tasks are processed concurrently.\n  for task in tasks:\n    handle = queue.enqueue(process_task, task)\n    task_handles.append(handle)\n  # Wait for each task to complete and retrieve its result.\n  # Return the results of all tasks.\n  return [handle.get_result() for handle in task_handles]\n'})}),"\n",(0,r.jsx)(n.h3,{id:"enqueueing-from-another-application",children:"Enqueueing from Another Application"}),"\n",(0,r.jsx)(n.p,{children:"Often, you want to enqueue a workflow from outside your DBOS application.\nFor example, let's say you have an API server and a data processing service.\nYou're using DBOS to build a durable data pipeline in the data processing service.\nWhen the API server receives a request, it should enqueue the data pipeline for execution on the data processing service."}),"\n",(0,r.jsxs)(n.p,{children:["You can use the ",(0,r.jsx)(n.a,{href:"/python/reference/client",children:"DBOS Client"})," to enqueue workflows from outside your DBOS application by connecting directly to your DBOS application's system database.\nSince the DBOS Client is designed to be used from outside your DBOS application, workflow and queue metadata must be specified explicitly."]}),"\n",(0,r.jsxs)(n.p,{children:["For example, this code enqueues the ",(0,r.jsx)(n.code,{children:"data_pipeline"})," workflow on the ",(0,r.jsx)(n.code,{children:"pipeline_queue"})," queue with ",(0,r.jsx)(n.code,{children:"task"})," as an argument."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from dbos import DBOSClient, EnqueueOptions\n\nclient = DBOSClient(system_database_url=os.environ["DBOS_SYSTEM_DATABASE_URL"])\n\noptions: EnqueueOptions = {\n  "queue_name": "pipeline_queue",\n  "workflow_name": "data_pipeline",\n}\nhandle = client.enqueue(options, task)\nresult = handle.get_result()\n'})}),"\n",(0,r.jsx)(n.h3,{id:"managing-concurrency",children:"Managing Concurrency"}),"\n",(0,r.jsx)(n.p,{children:"You can control how many workflows from a queue run simultaneously by configuring concurrency limits.\nThis helps prevent resource exhaustion when workflows consume significant memory or processing power."}),"\n",(0,r.jsx)(n.h4,{id:"worker-concurrency",children:"Worker Concurrency"}),"\n",(0,r.jsx)(n.p,{children:"Worker concurrency sets the maximum number of workflows from a queue that can run concurrently on a single DBOS process.\nThis is particularly useful for resource-intensive workflows to avoid exhausting the resources of any process.\nFor example, this queue has a worker concurrency of 5, so each process will run at most 5 workflows from this queue simultaneously:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from dbos import Queue\n\nqueue = Queue("example_queue", worker_concurrency=5)\n'})}),"\n",(0,r.jsxs)(n.p,{children:["Note that DBOS uses ",(0,r.jsx)(n.code,{children:"executor_id"})," to distinguish processes\u2014this is set automatically by Conductor and Cloud, but if those are not used it must be set to a unique value for each process through ",(0,r.jsx)(n.a,{href:"/python/reference/configuration",children:"configuration"}),"."]}),"\n",(0,r.jsx)(n.h4,{id:"global-concurrency",children:"Global Concurrency"}),"\n",(0,r.jsx)(n.p,{children:"Global concurrency limits the total number of workflows from a queue that can run concurrently across all DBOS processes in your application.\nFor example, this queue will have a maximum of 10 workflows running simultaneously across your entire application."}),"\n",(0,r.jsx)(n.admonition,{type:"warning",children:(0,r.jsxs)(n.p,{children:["Worker concurrency limits are recommended for most use cases.\nTake care when using a global concurrency limit as any ",(0,r.jsx)(n.code,{children:"PENDING"})," workflow on the queue counts toward the limit, including workflows from previous application versions"]})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from dbos import Queue\n\nqueue = Queue("example_queue", concurrency=10)\n'})}),"\n",(0,r.jsx)(n.h4,{id:"in-order-processing",children:"In-Order Processing"}),"\n",(0,r.jsxs)(n.p,{children:["You can use a queue with ",(0,r.jsx)(n.code,{children:"concurrency=1"})," to guarantee sequential, in-order processing of events.\nOnly a single event will be processed at a time.\nFor example, this app processes events sequentially in the order of their arrival:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from fastapi import FastAPI\nfrom dbos import DBOS, Queue\n\nqueue = Queue("in_order_queue", concurrency=1)\n\n@DBOS.step()\ndef process_event(event: str):\n   ...\n\ndef event_endpoint(event: str):\n   queue.enqueue(process_event, event)\n'})}),"\n",(0,r.jsx)(n.h3,{id:"rate-limiting",children:"Rate Limiting"}),"\n",(0,r.jsxs)(n.p,{children:["You can set ",(0,r.jsx)(n.em,{children:"rate limits"})," for a queue, limiting the number of functions that it can start in a given period.\nRate limits are global across all DBOS processes using this queue.\nFor example, this queue has a limit of 50 with a period of 30 seconds, so it may not start more than 50 functions in 30 seconds:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'queue = Queue("example_queue", limiter={"limit": 50, "period": 30})\n'})}),"\n",(0,r.jsx)(n.p,{children:"Rate limits are especially useful when working with a rate-limited API, such as many LLM APIs."}),"\n",(0,r.jsx)(n.h2,{id:"setting-timeouts",children:"Setting Timeouts"}),"\n",(0,r.jsxs)(n.p,{children:["You can set a timeout for an enqueued workflow with ",(0,r.jsx)(n.a,{href:"/python/reference/contexts#setworkflowtimeout",children:(0,r.jsx)(n.code,{children:"SetWorkflowTimeout"})}),".\nWhen the timeout expires, the workflow ",(0,r.jsx)(n.strong,{children:"and all its children"})," are cancelled.\nCancelling a workflow sets its status to ",(0,r.jsx)(n.code,{children:"CANCELLED"})," and preempts its execution at the beginning of its next step."]}),"\n",(0,r.jsxs)(n.p,{children:["Timeouts are ",(0,r.jsx)(n.strong,{children:"start-to-completion"}),": a workflow's timeout does not begin until the workflow is dequeued and starts execution.\nAlso, timeouts are ",(0,r.jsx)(n.strong,{children:"durable"}),": they are stored in the database and persist across restarts, so workflows can have very long timeouts."]}),"\n",(0,r.jsx)(n.p,{children:"Example syntax:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'@DBOS.workflow()\ndef example_workflow():\n    ...\n\nqueue = Queue("example-queue")\n\n# If the workflow does not complete within 10 seconds after being dequeued, it times out and is cancelled\nwith SetWorkflowTimeout(10):\n    queue.enqueue(example_workflow)\n'})}),"\n",(0,r.jsx)(n.h2,{id:"partitioning-queues",children:"Partitioning Queues"}),"\n",(0,r.jsxs)(n.p,{children:["You can ",(0,r.jsx)(n.strong,{children:"partition"}),' queues to distribute work across dynamically created queue partitions.\nWhen you enqueue a workflow on a partitioned queue, you must supply a queue partition key.\nPartitioned queues dequeue workflows and apply flow control limits for individual partitions, not for the entire queue.\nEssentially, you can think of each partition as a "subqueue" you dynamically create by enqueueing a workflow with a partition key.']}),"\n",(0,r.jsx)(n.p,{children:"For example, suppose you want your users to each be able to run at most one task at a time.\nYou can do this with a partitioned queue with a maximum concurrency limit of 1 where the partition key is user ID."}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Example Syntax"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'queue = Queue("partitioned_queue", partition_queue=True, concurrency=1)\n\n@DBOS.workflow()\ndef process_task(task: Task):\n  ...\n\n\ndef on_user_task_submission(user_id: str, task: Task):\n    # Partition the task queue by user ID. As the queue has a\n    # maximum concurrency of 1, this means that at most one\n    # task can run at once per user (but tasks from different\n    # users can run concurrently).\n    with SetEnqueueOptions(queue_partition_key=user_id):\n        queue.enqueue(process_task, task)\n'})}),"\n",(0,r.jsxs)(n.p,{children:["Sometimes, you want to apply global or per-worker limits to a partitioned queue.\nYou can do this with ",(0,r.jsx)(n.strong,{children:"multiple levels of queueing"}),'.\nCreate two queues: a partitioned queue with per-partition limits and a non-partitioned queue with global limits.\nEnqueue a "concurrency manager" workflow to the partitioned queue, which then enqueues your actual workflow\nto the non-partitioned queue and awaits its result.\nThis ensures both queues\' flow control limits are enforced on your workflow.\nFor example:']}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# By using two levels of queueing, we enforce both a concurrency limit of 1 on each partition\n# and a global concurrency limit of 5, meaning that no more than 5 tasks can run concurrently\n# across all partitions (and at most one task per partition).\nconcurrency_queue = Queue("concurrency-queue", concurrency=5)\npartitioned_queue = Queue("partitioned-queue", partition_queue=True, concurrency=1)\n\n@app.get("/queue")\n@DBOS.workflow()\ndef on_user_task_submission(user_id: str, task: Task):\n    # First, enqueue a "concurrency manager" workflow to the partitioned\n    # queue to enforce per-partition limits.\n    with SetEnqueueOptions(queue_partition_key=user_id):\n        partitioned_queue.enqueue(concurrency_manager, task)\n\n@DBOS.workflow()\ndef concurrency_manager(task):\n    # The "concurrency manager" workflow enqueues the process_task\n    # workflow on the non-partitioned queue and awaits its results\n    # to enforce global flow control limits.\n    return concurrency_queue.enqueue(process_task, task).get_result()\n\n@DBOS.workflow()\ndef process_task(task):\n    ...\n'})}),"\n",(0,r.jsx)(n.h2,{id:"deduplication",children:"Deduplication"}),"\n",(0,r.jsxs)(n.p,{children:["You can set a deduplication ID for an enqueued workflow with ",(0,r.jsx)(n.a,{href:"/python/reference/queues#setenqueueoptions",children:(0,r.jsx)(n.code,{children:"SetEnqueueOptions"})}),".\nAt any given time, only one workflow with a specific deduplication ID can be enqueued in the specified queue.\nIf a workflow with a deduplication ID is currently enqueued or actively executing (status ",(0,r.jsx)(n.code,{children:"ENQUEUED"})," or ",(0,r.jsx)(n.code,{children:"PENDING"}),"), subsequent workflow enqueue attempt with the same deduplication ID in the same queue will raise a ",(0,r.jsx)(n.code,{children:"DBOSQueueDeduplicatedError"})," exception."]}),"\n",(0,r.jsx)(n.p,{children:"For example, this is useful if you only want to have one workflow active at a time per user\u2014set the deduplication ID to the user's ID."}),"\n",(0,r.jsx)(n.p,{children:"Example syntax:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from dbos import DBOS, Queue, SetEnqueueOptions\nfrom dbos import error as dboserror\n\nqueue = Queue("example_queue")\n\nwith SetEnqueueOptions(deduplication_id="my_dedup_id"):\n    try:\n        handle = queue.enqueue(example_workflow, ...)\n    except dboserror.DBOSQueueDeduplicatedError as e:\n        # Handle deduplication error\n'})}),"\n",(0,r.jsx)(n.h2,{id:"priority",children:"Priority"}),"\n",(0,r.jsxs)(n.p,{children:["You can set a priority for an enqueued workflow with ",(0,r.jsx)(n.a,{href:"/python/reference/queues#setenqueueoptions",children:(0,r.jsx)(n.code,{children:"SetEnqueueOptions"})}),".\nWorkflows with the same priority are dequeued in ",(0,r.jsx)(n.strong,{children:"FIFO (first in, first out)"})," order. Priority values can range from ",(0,r.jsx)(n.code,{children:"1"})," to ",(0,r.jsx)(n.code,{children:"2,147,483,647"}),", where ",(0,r.jsx)(n.strong,{children:"a low number indicates a higher priority"}),".\nIf using priority, you must set ",(0,r.jsx)(n.code,{children:"priority_enabled=True"})," on your queue."]}),"\n",(0,r.jsx)(n.admonition,{type:"tip",children:(0,r.jsx)(n.p,{children:"Workflows without assigned priorities have the highest priority and are dequeued before workflows with assigned priorities."})}),"\n",(0,r.jsx)(n.p,{children:"Example syntax:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'queue = Queue("priority_queue", priority_enabled=True)\n\nwith SetEnqueueOptions(priority=10):\n    # All workflows are enqueued with priority set to 10\n    # They will be dequeued in FIFO order\n    for task in tasks:\n        queue.enqueue(task_workflow, task)\n\n# first_workflow (priority=1) will be dequeued before all task_workflows (priority=10)\nwith SetEnqueueOptions(priority=1):\n    queue.enqueue(first_workflow)\n'})})]})}function d(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(c,{...e})}):c(e)}},8453:(e,n,o)=>{o.d(n,{R:()=>s,x:()=>u});var t=o(6540);const r={},i=t.createContext(r);function s(e){const n=t.useContext(i);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function u(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:s(e.components),t.createElement(i.Provider,{value:n},e.children)}}}]);